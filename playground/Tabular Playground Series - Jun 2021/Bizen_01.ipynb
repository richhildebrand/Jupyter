{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/hiro5299834/tps06-1d-2dcnn-xgb-as-output-layer/\n",
    "\n",
    "# generates oof and pred for use in 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "272d2e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp39-cp39-manylinux2010_x86_64.whl (454.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 454.4 MB 48 kB/s  eta 0:00:011   |▉                               | 11.3 MB 3.8 MB/s eta 0:01:58     |█                               | 12.7 MB 15.6 MB/s eta 0:00:29     |█                               | 15.4 MB 15.6 MB/s eta 0:00:29     |█▍                              | 19.7 MB 15.6 MB/s eta 0:00:28     |█▌                              | 21.1 MB 15.6 MB/s eta 0:00:28     |██▋                             | 37.8 MB 10.8 MB/s eta 0:00:39     |███▎                            | 46.9 MB 10.8 MB/s eta 0:00:38     |█████▎                          | 74.3 MB 10.7 MB/s eta 0:00:36     |█████▌                          | 78.2 MB 10.7 MB/s eta 0:00:36     |██████▉                         | 97.0 MB 13.7 MB/s eta 0:00:27     |███████                         | 99.7 MB 13.7 MB/s eta 0:00:26     |████████▎                       | 117.0 MB 10.5 MB/s eta 0:00:33     |███████████                     | 156.5 MB 12.3 MB/s eta 0:00:25     |███████████▋                    | 165.1 MB 12.3 MB/s eta 0:00:24     |███████████▉                    | 168.1 MB 2.0 MB/s eta 0:02:20     |████████████▏                   | 172.2 MB 2.0 MB/s eta 0:02:18     |█████████████▏                  | 187.4 MB 9.1 MB/s eta 0:00:30     |██████████████                  | 199.8 MB 8.8 MB/s eta 0:00:29     |██████████████▏                 | 201.1 MB 9.8 MB/s eta 0:00:26     |██████████████▎                 | 202.9 MB 9.8 MB/s eta 0:00:26     |██████████████▋                 | 207.3 MB 9.8 MB/s eta 0:00:26     |███████████████▍                | 218.8 MB 9.6 MB/s eta 0:00:25     |███████████████▌                | 220.5 MB 10.5 MB/s eta 0:00:23     |███████████████▋                | 221.0 MB 10.5 MB/s eta 0:00:23     |███████████████▊                | 223.8 MB 10.5 MB/s eta 0:00:22     |████████████████                | 226.9 MB 10.5 MB/s eta 0:00:22     |█████████████████               | 243.0 MB 8.8 MB/s eta 0:00:25     |██████████████████▋             | 264.6 MB 17.4 MB/s eta 0:00:11     |███████████████████▍            | 275.5 MB 17.4 MB/s eta 0:00:11     |███████████████████▉            | 282.0 MB 14.5 MB/s eta 0:00:12     |████████████████████▌           | 291.7 MB 15.3 MB/s eta 0:00:11     |████████████████████▊           | 294.4 MB 15.3 MB/s eta 0:00:11     |█████████████████████           | 296.9 MB 15.3 MB/s eta 0:00:11     |█████████████████████▎          | 302.2 MB 15.3 MB/s eta 0:00:10     |███████████████████████         | 325.5 MB 20.6 MB/s eta 0:00:07     |████████████████████████▊       | 351.7 MB 9.1 MB/s eta 0:00:12     |█████████████████████████▏      | 357.4 MB 24.9 MB/s eta 0:00:04     |█████████████████████████▋      | 363.3 MB 24.9 MB/s eta 0:00:04     |█████████████████████████▋      | 364.0 MB 24.9 MB/s eta 0:00:04     |█████████████████████████▊      | 365.9 MB 24.9 MB/s eta 0:00:04     |███████████████████████████▋    | 391.9 MB 18.6 MB/s eta 0:00:04     |████████████████████████████▏   | 400.6 MB 7.1 MB/s eta 0:00:08     |█████████████████████████████   | 413.0 MB 10.2 MB/s eta 0:00:05     |████████████████████████████████| 453.3 MB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 1.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 13.7 MB/s eta 0:00:01     |██████▋                         | 256 kB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 8.8 MB/s eta 0:00:01     |██████████                      | 1.4 MB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 3.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 8.4 MB/s eta 0:00:01    |████████████                    | 5.6 MB 9.1 MB/s eta 0:00:02     |██████████████                  | 6.5 MB 9.1 MB/s eta 0:00:01     |█████████████████████████████▏  | 13.6 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 753 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.17.2)\n",
      "Collecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp39-cp39-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.36.2)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 19.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.33.1-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow) (2.26.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 5.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow) (49.6.0.post20210108)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 3.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=1b2abc172a27ea7f29ab891757e715c575cc35e2019fd84ef5c1c005605251ee\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp39-cp39-linux_x86_64.whl size=36920 sha256=38bfa2ef7e172e86438c5c6a823fc2ccf423a627f3a313d8d38b40f158622174\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/98/23/68/efe259aaca055e93b08e74fbe512819c69a2155c11ba3c0f10\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: pyasn1, six, rsa, pyasn1-modules, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-nightly, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.1\n",
      "    Uninstalling numpy-1.21.1:\n",
      "      Successfully uninstalled numpy-1.21.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Uninstalling typing-extensions-3.10.0.0:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.3.0\n",
      "    Uninstalling h5py-3.3.0:\n",
      "      Successfully uninstalled h5py-3.3.0\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 flatbuffers-1.12 gast-0.4.0 google-auth-1.33.1 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 six-1.15.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 werkzeug-2.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6964b7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.13.0-cp39-cp39-manylinux2010_x86_64.whl (679 kB)\n",
      "\u001b[K     |████████████████████████████████| 679 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typeguard>=2.7\n",
      "  Downloading typeguard-2.12.1-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.13.0 typeguard-2.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f296d17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-23 15:09:37.519820: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-23 15:09:37.519882: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_addons'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_57/3729665210.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_addons'"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "219773f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'debug': False,\n",
    "    'target': 'target',\n",
    "    'n_class': 9,\n",
    "    'seed': 299792458,\n",
    "    'seed_l': [299, 792, 458],\n",
    "    'k': [5, 2],\n",
    "    'n_clusters': [9, 2],\n",
    "    'n_components': [9, 2],\n",
    "    'emb_out_dim': 16,\n",
    "    'sigma': [0.7, 0.9],\n",
    "    'n_conv2d': [1, 3],\n",
    "    'max_epochs': 100,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 1e-3,\n",
    "    'es_patience': 10,\n",
    "    'lr_patience': 2,\n",
    "    'lr_factor': 0.7,\n",
    "    'n_splits': 10,\n",
    "    'nn_verbose': 0,\n",
    "    'save_path': './outputs/',\n",
    "    'n_estimators': 3000,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'gbt_verbose': 0,\n",
    "}\n",
    "\n",
    "if CFG['debug']:\n",
    "    CFG['max_epochs'] = 2\n",
    "    CFG['n_splits'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0fffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=2021):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(CFG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd8b0928",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CFG['save_path'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00307e17",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee6134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "target_ohe = pd.get_dummies(train[CFG['target']])\n",
    "target = train[CFG['target']].apply(lambda x: int(x.split(\"_\")[-1])-1)\n",
    "features = [col for col in train.columns if col.startswith('feature_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "972203b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "all_df = pd.concat([train, test]).reset_index(drop=True)\n",
    "all_df = scaler.fit_transform(all_df[features])\n",
    "train_2d = all_df[:train.shape[0]].reshape(-1, 5, 5, 3)\n",
    "test_2d = all_df[train.shape[0]:].reshape(-1, 5, 5, 3)\n",
    "\n",
    "train_km, train_pca, train_knn = [], [], []\n",
    "test_km, test_pca, test_knn = [], [], []\n",
    "\n",
    "for n_model in range(len(CFG['k'])):\n",
    "    km = KMeans(n_clusters=CFG['n_clusters'][n_model], random_state=CFG['seed'])\n",
    "    all_km = km.fit_transform(all_df)\n",
    "    train_km.append(all_km[:train.shape[0]])\n",
    "    test_km.append(all_km[train.shape[0]:])\n",
    "\n",
    "    pca = PCA(n_components=CFG['n_components'][n_model], random_state=CFG['seed'])\n",
    "    all_pca = pca.fit_transform(all_df)\n",
    "    train_pca.append(all_pca[:train.shape[0]])\n",
    "    test_pca.append(all_pca[train.shape[0]:])\n",
    "    \n",
    "    all_knn = np.concatenate([\n",
    "        np.load(f\"../input/tps06-knn-features/add_feat_k{CFG['k'][n_model]}_train.npy\"),\n",
    "        np.load(f\"../input/tps06-knn-features/add_feat_k{CFG['k'][n_model]}_test.npy\")\n",
    "        ])\n",
    "    all_knn = scaler.fit_transform(all_knn)\n",
    "    train_knn.append(all_knn[:train.shape[0]])\n",
    "    test_knn.append(all_knn[train.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d088439",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_in_dim = pd.concat([train, test]).reset_index(drop=True)[features].max().max()+1\n",
    "emb_out_dim = CFG['emb_out_dim']\n",
    "emb_dims = [emb_in_dim, emb_out_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c49dee",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "601906f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4eeeb1f31345c8a9256846dffd35c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_model(shape, emb_dims, sigma, n_conv2d, n_model):\n",
    "    #--------------------------------------\n",
    "    conv_inputs = layers.Input(shape=shape[0])\n",
    "    conv2_inputs = layers.Input(shape=shape[1])\n",
    "    knn_inputs = layers.Input(shape=shape[2])\n",
    "    kms_inputs = layers.Input(shape=shape[3])\n",
    "    pca_inputs = layers.Input(shape=shape[4])\n",
    "    \n",
    "    #----------- Embedding layers ----------------------\n",
    "    embed = layers.Embedding(\n",
    "        input_dim=emb_dims[0], \n",
    "        output_dim=emb_dims[1],\n",
    "        embeddings_regularizer='l2'\n",
    "        )(conv_inputs)\n",
    "\n",
    "    #----------- Convolution1 layers ----------------------\n",
    "    embed = layers.Conv1D(8, 1, activation='relu')(embed)\n",
    "    embed = layers.Flatten()(embed)\n",
    "    hidden_emb = layers.Dropout(0.4)(embed)\n",
    "\n",
    "    #----------- Convolution2 layers ----------------------\n",
    "    cnv2 = layers.SeparableConv2D(8, 3, padding='same', activation='relu')(conv2_inputs)\n",
    "    cnv2 = layers.BatchNormalization()(cnv2)\n",
    "    \n",
    "    for _ in range(n_conv2d[n_model]-1):\n",
    "        cnv2 = layers.SeparableConv2D(8, 3, padding='same', activation='relu')(cnv2)\n",
    "        cnv2 = layers.BatchNormalization()(cnv2)\n",
    "\n",
    "    cnv2 = layers.Flatten()(cnv2)\n",
    "    hidden_cnv2 = layers.Dropout(0.4)(cnv2)\n",
    "\n",
    "    #----------- Residual blocks layers ----------------------\n",
    "    hidden_emb = tfa.layers.NoisyDense(units=16, sigma=sigma[n_model], activation='relu')(hidden_emb)\n",
    "    hidden_emb = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "            units=16,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal'\n",
    "            ))(hidden_emb)\n",
    "    \n",
    "    hidden_cnv2 = tfa.layers.NoisyDense(units=16, sigma=sigma[n_model], activation='relu')(hidden_cnv2)\n",
    "    hidden_cnv2 = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "            units=16,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal'\n",
    "            ))(hidden_cnv2)\n",
    "\n",
    "    hidden = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden_emb, hidden_cnv2, knn_inputs, kms_inputs, pca_inputs]))\n",
    "    hidden = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "            units=16,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal'\n",
    "        ))(hidden)\n",
    "    hidden = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden_emb, hidden_cnv2, knn_inputs, kms_inputs, pca_inputs, hidden]))\n",
    "\n",
    "    hidden2 = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "            units=16,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal'\n",
    "        ))(hidden)\n",
    "    hidden2 = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden_emb, hidden_cnv2, knn_inputs, kms_inputs, pca_inputs, hidden, hidden2]))\n",
    "\n",
    "    hidden_out= tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "            units=16,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal'\n",
    "        ))(hidden2)\n",
    "\n",
    "    #----------- Final layer -----------------------\n",
    "    conv_outputs = layers.Dense(\n",
    "        units=9, \n",
    "        activation='softmax',\n",
    "        kernel_initializer='lecun_normal')(hidden_out)\n",
    "    \n",
    "    #----------- Model instantiation  ---------------\n",
    "    model = Model([conv_inputs, conv2_inputs, knn_inputs, kms_inputs, pca_inputs], conv_outputs)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=tfa.optimizers.LazyAdam(learning_rate=CFG['learning_rate'], amsgrad=False), \n",
    "        metrics=custom_metric\n",
    "    )\n",
    "    \n",
    "    #----------- Model instantiation  ---------------\n",
    "    hidden_model = Model([conv_inputs, conv2_inputs, knn_inputs, kms_inputs, pca_inputs], hidden_out)\n",
    "    \n",
    "    return model, hidden_model\n",
    "\n",
    "for i in range(len(CFG['k'])):\n",
    "    shape = [\n",
    "        train[features].shape[1],\n",
    "        train_2d.shape[1:],\n",
    "        CFG['k'][i]*CFG['n_class'],\n",
    "        CFG['n_clusters'][i],\n",
    "        CFG['n_components'][i]\n",
    "    ]\n",
    "    model, hidden_model = create_model(shape, emb_dims, CFG['sigma'], CFG['n_conv2d'], i)\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f13914",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 5e-3,\n",
    "    'colsample_bytree': 0.4,\n",
    "    'subsample': 0.6,\n",
    "    'reg_alpha': 6,\n",
    "    'min_child_weight': 100,\n",
    "    'n_jobs': -1,\n",
    "    'num_class': CFG['n_class'],\n",
    "    'seed': CFG['seed'],\n",
    "    'tree_method': 'gpu_hist',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bda3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_oof = np.zeros((len(CFG['k']), len(CFG['seed_l']), train.shape[0], CFG['n_class']))\n",
    "gbt_oof = np.zeros((len(CFG['k']), len(CFG['seed_l']), train.shape[0], CFG['n_class']))\n",
    "nn_pred = np.zeros((len(CFG['k']), len(CFG['seed_l']), test.shape[0], CFG['n_class']))\n",
    "gbt_pred = np.zeros((len(CFG['k']), len(CFG['seed_l']), test.shape[0], CFG['n_class']))\n",
    "eval_fold_result = {}\n",
    "\n",
    "for n_model in range(len(CFG['k'])):\n",
    "    print(f\"===== MODEL {n_model} cross validation =====\")\n",
    "\n",
    "    for n_seed, seed in enumerate(CFG['seed_l']):\n",
    "    \n",
    "        skf = StratifiedKFold(n_splits=CFG['n_splits'], shuffle=True, random_state=seed)\n",
    "\n",
    "        for fold, (trn_idx, val_idx) in enumerate(skf.split(train, train[CFG['target']])):\n",
    "            X_train, y_train_ohe, y_train = train[features].iloc[trn_idx], target_ohe.iloc[trn_idx], target.iloc[trn_idx]\n",
    "            X_valid, y_valid_ohe, y_valid = train[features].iloc[val_idx], target_ohe.iloc[val_idx], target.iloc[val_idx]\n",
    "            X_test = test[features]\n",
    "\n",
    "            X_train_2d, X_valid_2d = train_2d[trn_idx], train_2d[val_idx]\n",
    "            X_test_2d = test_2d\n",
    "            \n",
    "            X_train_knn, X_valid_knn = train_knn[n_model][trn_idx], train_knn[n_model][val_idx]\n",
    "            X_test_knn = test_knn[n_model]\n",
    "\n",
    "            X_train_km, X_valid_km = train_km[n_model][trn_idx], train_km[n_model][val_idx]\n",
    "            X_test_km = test_km[n_model]\n",
    "\n",
    "            X_train_pca, X_valid_pca = train_pca[n_model][trn_idx], train_pca[n_model][val_idx]\n",
    "            X_test_pca = test_pca[n_model]\n",
    "\n",
    "            log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            tb_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "            K.clear_session()  \n",
    "            shape = [\n",
    "                X_train.shape[1],\n",
    "                X_train_2d.shape[1:],\n",
    "                CFG['k'][n_model]*CFG['n_class'],\n",
    "                CFG['n_clusters'][n_model],\n",
    "                CFG['n_components'][n_model]\n",
    "            ]\n",
    "\n",
    "            model, hidden_model = create_model(shape, emb_dims, CFG['sigma'], CFG['n_conv2d'], n_model)\n",
    "            model.fit(\n",
    "                [X_train, X_train_2d, X_train_knn, X_train_km, X_train_pca], y_train_ohe,\n",
    "                batch_size=CFG['batch_size'],\n",
    "                epochs=CFG['max_epochs'],\n",
    "                validation_data=([X_valid, X_valid_2d, X_valid_knn, X_valid_km, X_valid_pca], y_valid_ohe),\n",
    "                callbacks=[es_cb, sch_cb, tb_cb],\n",
    "                verbose=CFG['nn_verbose']\n",
    "            )\n",
    "\n",
    "            nn_oof[n_model, n_seed, val_idx] = model.predict([X_valid, X_valid_2d, X_valid_knn, X_valid_km, X_valid_pca])\n",
    "            nn_pred[n_model, n_seed] += model.predict([X_test, X_test_2d, X_test_knn, X_test_km, X_test_pca]) / CFG['n_splits']\n",
    "            m_logloss = log_loss(y_valid_ohe, nn_oof[n_model, n_seed, val_idx])\n",
    "            print(f\"nn  model{n_model} seed{seed} fold{fold}: m_logloss {m_logloss}\")\n",
    "            \n",
    "            hidden_train = hidden_model.predict([X_train, X_train_2d, X_train_knn, X_train_km, X_train_pca])\n",
    "            hidden_valid = hidden_model.predict([X_valid, X_valid_2d, X_valid_knn, X_valid_km, X_valid_pca])\n",
    "            hidden_test = hidden_model.predict([X_test, X_test_2d, X_test_knn, X_test_km, X_test_pca])\n",
    "            trn_data = xgb.DMatrix(data=hidden_train, label=y_train)\n",
    "            val_data = xgb.DMatrix(data=hidden_valid, label=y_valid)\n",
    "\n",
    "            xgb_params['seed'] = seed\n",
    "            gbt_model = xgb.train(\n",
    "                params=xgb_params,\n",
    "                dtrain=trn_data,\n",
    "                evals=[(trn_data, \"train\"), (val_data, \"valid\")],\n",
    "                evals_result=eval_fold_result,\n",
    "                num_boost_round = CFG['n_estimators'],\n",
    "                verbose_eval=CFG['gbt_verbose'],\n",
    "                early_stopping_rounds=CFG['early_stopping_rounds'],\n",
    "                )\n",
    "        \n",
    "            gbt_oof[n_model, n_seed, val_idx] = gbt_model.predict(xgb.DMatrix(hidden_valid), ntree_limit=gbt_model.best_ntree_limit)\n",
    "            gbt_pred[n_model, n_seed] += gbt_model.predict(xgb.DMatrix(hidden_test), ntree_limit=gbt_model.best_ntree_limit) / CFG['n_splits']\n",
    "            m_logloss = log_loss(y_valid, gbt_oof[n_model, n_seed, val_idx])\n",
    "            print(f\"xgb model{n_model} seed{seed} fold{fold}: m_logloss {m_logloss}\")\n",
    "        \n",
    "        print(\"-\"*60)\n",
    "        m_logloss = log_loss(target, nn_oof[n_model, n_seed])\n",
    "        print(f\"nn  model{n_model} seed{seed}: m_logloss {m_logloss}\")\n",
    "        m_logloss = log_loss(target, gbt_oof[n_model, n_seed])\n",
    "        print(f\"xgb model{n_model} seed{seed}: m_logloss {m_logloss}\\n\")\n",
    "\n",
    "        np.save(CFG['save_path'] + f\"nn_model{n_model}_seed{seed}_oof\", nn_oof[n_model, n_seed])\n",
    "        np.save(CFG['save_path'] + f\"nn_model{n_model}_seed{seed}_pred\", nn_pred[n_model, n_seed])\n",
    "        np.save(CFG['save_path'] + f\"xgb_model{n_model}_seed{seed}_oof\", gbt_oof[n_model, n_seed])\n",
    "        np.save(CFG['save_path'] + f\"xgb_model{n_model}_seed{seed}_pred\", gbt_pred[n_model, n_seed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3246acb",
   "metadata": {},
   "source": [
    "## Post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421298da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_optimizer(X, a0, a1, a2, a3, a4, a5, a6, a7, a8):\n",
    "    oof = np.array([X[0]*a0, X[1]*a1, X[2]*a2, X[3]*a3, X[4]*a4, X[5]*a5, X[6]*a6, X[7]*a7, X[8]*a8]).transpose()\n",
    "    oof = oof / np.sum(oof, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    return log_loss(target, oof)\n",
    "\n",
    "def get_optimized(X, vals):\n",
    "    opt_val = 0\n",
    "    for i, val in enumerate(vals):\n",
    "        if i != len(X):\n",
    "            opt_val += X[i]*val\n",
    "        else:\n",
    "            coef = 1\n",
    "            for j in range(i):\n",
    "                coef -= X[j]\n",
    "            opt_val += coef*val\n",
    "\n",
    "    return opt_val\n",
    "\n",
    "def model_optimizer(X, oofs):\n",
    "    opt_oof = get_optimized(X, oofs)\n",
    "    \n",
    "    return log_loss(target, opt_oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e37145",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = np.concatenate([nn_oof, gbt_oof])\n",
    "pred = np.concatenate([nn_pred, gbt_pred])\n",
    "\n",
    "res_l = []\n",
    "for idx0 in tqdm(range(oof.shape[0])):\n",
    "    for idx1 in tqdm(range(oof.shape[1]), leave=False):\n",
    "        res = minimize(\n",
    "            fun=class_optimizer,\n",
    "            x0=[1.0 for _ in range(CFG['n_class'])],\n",
    "            args=tuple(oof[idx0, idx1, :, i] for i in range(CFG['n_class'])),\n",
    "            method='Nelder-Mead',\n",
    "            options={'maxiter': 300})\n",
    "\n",
    "        oof[idx0, idx1] = np.array([res.x[i]*oof[idx0, idx1, :, i] for i in range(CFG['n_class'])]).transpose()\n",
    "        oof[idx0, idx1] = oof[idx0, idx1] / np.sum(oof[idx0, idx1], axis=1).reshape(-1, 1)\n",
    "\n",
    "        pred[idx0, idx1] = np.array([res.x[i]*pred[idx0, idx1, :, i] for i in range(CFG['n_class'])]).transpose()\n",
    "        pred[idx0, idx1] = pred[idx0, idx1] / np.sum(pred[idx0, idx1], axis=1).reshape(-1, 1)\n",
    "\n",
    "        res_l.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20051eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_oof = np.mean(np.array(oof), axis=1)\n",
    "avg_pred = np.mean(np.array(pred), axis=1)\n",
    "\n",
    "res = minimize(\n",
    "    fun=model_optimizer,\n",
    "    x0=[1/oof.shape[0] for _ in range(avg_oof.shape[0]-1)],\n",
    "    args=tuple([avg_oof]),\n",
    "    method='Nelder-Mead',\n",
    "    options={'maxiter': 1000})\n",
    "\n",
    "opt_oof = get_optimized(res.x, avg_oof)\n",
    "opt_pred = get_optimized(res.x, avg_pred)\n",
    "\n",
    "print(f\"logloss score: {log_loss(target, opt_oof)}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0c6fb",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3929236e",
   "metadata": {},
   "source": [
    "### Target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c97894",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4), tight_layout=True)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "target.hist()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "pd.Series(opt_oof.argmax(axis=1)).hist()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "pd.Series(opt_pred.argmax(axis=1)).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244a3d9",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(target, opt_oof.argmax(axis=1))\n",
    "\n",
    "plt.figure(figsize=((16,4)))\n",
    "sns.heatmap(cm, annot=True, fmt='5d', cmap='Blues')\n",
    "plt.savefig(\"confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb17f5c",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(target, opt_oof.argmax(axis=1), digits=4))\n",
    "\n",
    "report = pd.DataFrame(classification_report(target, opt_oof.argmax(axis=1), digits=4, output_dict=True)).transpose()\n",
    "report.to_csv(\"report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a16abe",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9adf99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:, 1:] = opt_pred  \n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8), tight_layout=True)\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.title(f\"Class_{i+1}\")\n",
    "    submission[f'Class_{i+1}'].hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
