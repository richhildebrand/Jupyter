{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207a8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/hiro5299834/tps06-1d-2dcnn-xgb-as-output-layer/\n",
    "\n",
    "# generates oof and pred for use in 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272d2e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.9/site-packages (2.5.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.17.2)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow) (1.33.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow) (49.6.0.post20210108)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6964b7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in /opt/conda/lib/python3.9/site-packages (0.13.0)\r\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.9/site-packages (from tensorflow_addons) (2.12.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0122eabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.4.2-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 166.7 MB 8.2 MB/s eta 0:00:012  |                                | 358 kB 5.9 MB/s eta 0:00:29     |▎                               | 1.3 MB 5.9 MB/s eta 0:00:28     |██████▊                         | 35.0 MB 10.8 MB/s eta 0:00:13     |██████▉                         | 35.6 MB 10.8 MB/s eta 0:00:13     |███████▋                        | 39.6 MB 9.3 MB/s eta 0:00:14     |█████████▊                      | 50.9 MB 10.2 MB/s eta 0:00:12     |█████████████████████████▋      | 133.7 MB 6.4 MB/s eta 0:00:06     |████████████████████████████▏   | 146.5 MB 10.0 MB/s eta 0:00:03     |████████████████████████████▋   | 148.8 MB 6.8 MB/s eta 0:00:03     |████████████████████████████▉   | 150.2 MB 6.8 MB/s eta 0:00:03     |███████████████████████████████▏| 162.3 MB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from xgboost) (1.7.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from xgboost) (1.19.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f296d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "219773f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'debug': False,\n",
    "    'target': 'target',\n",
    "    'n_class': 9,\n",
    "    'seed': 299792458,\n",
    "    'seed_l': [299, 792, 458],\n",
    "    'k': [5, 2],\n",
    "    'n_clusters': [9, 2],\n",
    "    'n_components': [9, 2],\n",
    "    'emb_out_dim': 16,\n",
    "    'sigma': [0.7, 0.9],\n",
    "    'n_conv2d': [1, 3],\n",
    "    'max_epochs': 100,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 1e-3,\n",
    "    'es_patience': 10,\n",
    "    'lr_patience': 2,\n",
    "    'lr_factor': 0.7,\n",
    "    'n_splits': 10,\n",
    "    'nn_verbose': 0,\n",
    "    'save_path': './outputs/',\n",
    "    'n_estimators': 3000,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'gbt_verbose': 0,\n",
    "}\n",
    "\n",
    "if CFG['debug']:\n",
    "    CFG['max_epochs'] = 2\n",
    "    CFG['n_splits'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e0fffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=2021):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(CFG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd8b0928",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CFG['save_path'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00307e17",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eee6134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "target_ohe = pd.get_dummies(train[CFG['target']])\n",
    "target = train[CFG['target']].apply(lambda x: int(x.split(\"_\")[-1])-1)\n",
    "features = [col for col in train.columns if col.startswith('feature_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "972203b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "all_df = pd.concat([train, test]).reset_index(drop=True)\n",
    "all_df = scaler.fit_transform(all_df[features])\n",
    "train_2d = all_df[:train.shape[0]].reshape(-1, 5, 5, 3)\n",
    "test_2d = all_df[train.shape[0]:].reshape(-1, 5, 5, 3)\n",
    "\n",
    "train_km, train_pca, train_knn = [], [], []\n",
    "test_km, test_pca, test_knn = [], [], []\n",
    "\n",
    "for n_model in range(len(CFG['k'])):\n",
    "    km = KMeans(n_clusters=CFG['n_clusters'][n_model], random_state=CFG['seed'])\n",
    "    all_km = km.fit_transform(all_df)\n",
    "    train_km.append(all_km[:train.shape[0]])\n",
    "    test_km.append(all_km[train.shape[0]:])\n",
    "\n",
    "    pca = PCA(n_components=CFG['n_components'][n_model], random_state=CFG['seed'])\n",
    "    all_pca = pca.fit_transform(all_df)\n",
    "    train_pca.append(all_pca[:train.shape[0]])\n",
    "    test_pca.append(all_pca[train.shape[0]:])\n",
    "    \n",
    "    all_knn = np.concatenate([\n",
    "        np.load(f\"./knn_saved/add_feat_k{CFG['k'][n_model]}_train.npy\"),\n",
    "        np.load(f\"./knn_saved/add_feat_k{CFG['k'][n_model]}_test.npy\")\n",
    "        ])\n",
    "    all_knn = scaler.fit_transform(all_knn)\n",
    "    train_knn.append(all_knn[:train.shape[0]])\n",
    "    test_knn.append(all_knn[train.shape[0]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c0cd4",
   "metadata": {},
   "source": [
    "## Train and Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8814f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_metric(y_true, y_pred):\n",
    "    y_pred = K.clip(y_pred, 1e-15, 1-1e-15)\n",
    "    loss = K.mean(cce(y_true, y_pred))\n",
    "    return loss\n",
    "\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "es_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_custom_metric',\n",
    "    min_delta=1e-05,\n",
    "    patience=CFG['es_patience'],\n",
    "    verbose=CFG['nn_verbose'],\n",
    "    mode='min',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "sch_cb = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_custom_metric',\n",
    "    factor=CFG['lr_factor'],\n",
    "    patience=CFG['lr_patience'],\n",
    "    min_lr=1e-5,\n",
    "    verbose=CFG['nn_verbose'],\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d088439",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_in_dim = pd.concat([train, test]).reset_index(drop=True)[features].max().max()+1\n",
    "emb_out_dim = CFG['emb_out_dim']\n",
    "emb_dims = [emb_in_dim, emb_out_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c49dee",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "601906f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 5, 5, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 75, 16)       5648        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 5, 5, 8)      59          input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 75, 8)        136         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 5, 5, 8)      32          separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 600)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 200)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 600)          0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 200)          0           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "noisy_dense_2 (NoisyDense)      (None, 16)           19232       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "noisy_dense_3 (NoisyDense)      (None, 16)           6432        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_5 (WeightN (None, 16)           561         noisy_dense_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_6 (WeightN (None, 16)           561         noisy_dense_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, 9)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 9)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 695)          0           flatten_2[0][0]                  \n",
      "                                                                 weight_normalization_5[0][0]     \n",
      "                                                                 weight_normalization_6[0][0]     \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 695)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_7 (WeightN (None, 16)           22289       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 711)          0           flatten_2[0][0]                  \n",
      "                                                                 weight_normalization_5[0][0]     \n",
      "                                                                 weight_normalization_6[0][0]     \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "                                                                 weight_normalization_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 711)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_8 (WeightN (None, 16)           22801       dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1422)         0           flatten_2[0][0]                  \n",
      "                                                                 weight_normalization_5[0][0]     \n",
      "                                                                 weight_normalization_6[0][0]     \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 weight_normalization_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1422)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_9 (WeightN (None, 16)           45553       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 9)            153         weight_normalization_9[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 123,457\n",
      "Trainable params: 77,596\n",
      "Non-trainable params: 45,861\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 5, 5, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 5, 5, 8)      59          input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 5, 5, 8)      32          separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 5, 5, 8)      144         batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 5, 5, 8)      32          separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 75, 16)       5648        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 5, 5, 8)      144         batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 75, 8)        136         embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 5, 5, 8)      32          separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 600)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 200)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 600)          0           flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 200)          0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "noisy_dense_4 (NoisyDense)      (None, 16)           19232       dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "noisy_dense_5 (NoisyDense)      (None, 16)           6432        dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_10 (Weight (None, 16)           561         noisy_dense_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_11 (Weight (None, 16)           561         noisy_dense_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 18)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 654)          0           flatten_4[0][0]                  \n",
      "                                                                 weight_normalization_10[0][0]    \n",
      "                                                                 weight_normalization_11[0][0]    \n",
      "                                                                 input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 654)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_12 (Weight (None, 16)           20977       dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 670)          0           flatten_4[0][0]                  \n",
      "                                                                 weight_normalization_10[0][0]    \n",
      "                                                                 weight_normalization_11[0][0]    \n",
      "                                                                 input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 weight_normalization_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 670)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_13 (Weight (None, 16)           21489       dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1340)         0           flatten_4[0][0]                  \n",
      "                                                                 weight_normalization_10[0][0]    \n",
      "                                                                 weight_normalization_11[0][0]    \n",
      "                                                                 input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 dropout_13[0][0]                 \n",
      "                                                                 weight_normalization_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 1340)         0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_14 (Weight (None, 16)           42929       dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 9)            153         weight_normalization_14[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 118,561\n",
      "Trainable params: 75,292\n",
      "Non-trainable params: 43,269\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def create_model(shape, emb_dims, sigma, n_conv2d, n_model):\n",
    "    #--------------------------------------\n",
    "    conv_inputs = layers.Input(shape=shape[0])\n",
    "    conv2_inputs = layers.Input(shape=shape[1])\n",
    "    knn_inputs = layers.Input(shape=shape[2])\n",
    "    kms_inputs = layers.Input(shape=shape[3])\n",
    "    pca_inputs = layers.Input(shape=shape[4])\n",
    "    \n",
    "    #----------- Embedding layers ----------------------\n",
    "    embed = layers.Embedding(\n",
    "        input_dim=emb_dims[0], \n",
    "        output_dim=emb_dims[1],\n",
    "        embeddings_regularizer='l2'\n",
    "        )(conv_inputs)\n",
    "\n",
    "    #----------- Convolution1 layers ----------------------\n",
    "    embed = layers.Conv1D(8, 1, activation='relu')(embed)\n",
    "    embed = layers.Flatten()(embed)\n",
    "    hidden_emb = layers.Dropout(0.4)(embed)\n",
    "\n",
    "    #----------- Convolution2 layers ----------------------\n",
    "    cnv2 = layers.SeparableConv2D(8, 3, padding='same', activation='relu')(conv2_inputs)\n",
    "    cnv2 = layers.BatchNormalization()(cnv2)\n",
    "    \n",
    "    for _ in range(n_conv2d[n_model]-1):\n",
    "        cnv2 = layers.SeparableConv2D(8, 3, padding='same', activation='relu')(cnv2)\n",
    "        cnv2 = layers.BatchNormalization()(cnv2)\n",
    "\n",
    "    cnv2 = layers.Flatten()(cnv2)\n",
    "    hidden_cnv2 = layers.Dropout(0.4)(cnv2)\n",
    "\n",
    "    #----------- Residual blocks layers ----------------------\n",
    "    hidden_emb = tfa.layers.NoisyDense(units=16, sigma=sigma[n_model], activation='relu')(hidden_emb)\n",
    "    hidden_emb = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "            units=16,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal'\n",
    "            ))(hidden_emb)\n",
    "    \n",
    "    hidden_cnv2 = tfa.layers.NoisyDense(units=16, sigma=sigma[n_model], activation='relu')(hidden_cnv2)\n",
    "    hidden_cnv2 = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "            units=16,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal'\n",
    "            ))(hidden_cnv2)\n",
    "\n",
    "    hidden = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden_emb, hidden_cnv2, knn_inputs, kms_inputs, pca_inputs]))\n",
    "    hidden = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "            units=16,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal'\n",
    "        ))(hidden)\n",
    "    hidden = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden_emb, hidden_cnv2, knn_inputs, kms_inputs, pca_inputs, hidden]))\n",
    "\n",
    "    hidden2 = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "            units=16,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal'\n",
    "        ))(hidden)\n",
    "    hidden2 = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden_emb, hidden_cnv2, knn_inputs, kms_inputs, pca_inputs, hidden, hidden2]))\n",
    "\n",
    "    hidden_out= tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "            units=16,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal'\n",
    "        ))(hidden2)\n",
    "\n",
    "    #----------- Final layer -----------------------\n",
    "    conv_outputs = layers.Dense(\n",
    "        units=9, \n",
    "        activation='softmax',\n",
    "        kernel_initializer='lecun_normal')(hidden_out)\n",
    "    \n",
    "    #----------- Model instantiation  ---------------\n",
    "    model = Model([conv_inputs, conv2_inputs, knn_inputs, kms_inputs, pca_inputs], conv_outputs)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=tfa.optimizers.LazyAdam(learning_rate=CFG['learning_rate'], amsgrad=False), \n",
    "        metrics=custom_metric\n",
    "    )\n",
    "    \n",
    "    #----------- Model instantiation  ---------------\n",
    "    hidden_model = Model([conv_inputs, conv2_inputs, knn_inputs, kms_inputs, pca_inputs], hidden_out)\n",
    "    \n",
    "    return model, hidden_model\n",
    "\n",
    "for i in range(len(CFG['k'])):\n",
    "    shape = [\n",
    "        train[features].shape[1],\n",
    "        train_2d.shape[1:],\n",
    "        CFG['k'][i]*CFG['n_class'],\n",
    "        CFG['n_clusters'][i],\n",
    "        CFG['n_components'][i]\n",
    "    ]\n",
    "    model, hidden_model = create_model(shape, emb_dims, CFG['sigma'], CFG['n_conv2d'], i)\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2f13914",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 5e-3,\n",
    "    'colsample_bytree': 0.4,\n",
    "    'subsample': 0.6,\n",
    "    'reg_alpha': 6,\n",
    "    'min_child_weight': 100,\n",
    "    'n_jobs': -1,\n",
    "    'num_class': CFG['n_class'],\n",
    "    'seed': CFG['seed'],\n",
    "    'tree_method': 'gpu_hist',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bda3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== MODEL 0 cross validation =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-24 15:50:16.670018: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2021-07-24 15:50:16.670095: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2021-07-24 15:50:16.670146: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2021-07-24 15:50:20.125514: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2021-07-24 15:50:20.125592: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2021-07-24 15:50:20.153224: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-07-24 15:50:20.160024: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2021-07-24 15:50:20.197261: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/fit/20210724-155016/train/plugins/profile/2021_07_24_15_50_20\n",
      "2021-07-24 15:50:20.205648: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to logs/fit/20210724-155016/train/plugins/profile/2021_07_24_15_50_20/f8f7c4f35391.trace.json.gz\n",
      "2021-07-24 15:50:20.221903: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/fit/20210724-155016/train/plugins/profile/2021_07_24_15_50_20\n",
      "2021-07-24 15:50:20.227632: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210724-155016/train/plugins/profile/2021_07_24_15_50_20/f8f7c4f35391.memory_profile.json.gz\n",
      "2021-07-24 15:50:20.256217: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/fit/20210724-155016/train/plugins/profile/2021_07_24_15_50_20Dumped tool data for xplane.pb to logs/fit/20210724-155016/train/plugins/profile/2021_07_24_15_50_20/f8f7c4f35391.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/fit/20210724-155016/train/plugins/profile/2021_07_24_15_50_20/f8f7c4f35391.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/fit/20210724-155016/train/plugins/profile/2021_07_24_15_50_20/f8f7c4f35391.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/fit/20210724-155016/train/plugins/profile/2021_07_24_15_50_20/f8f7c4f35391.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/fit/20210724-155016/train/plugins/profile/2021_07_24_15_50_20/f8f7c4f35391.kernel_stats.pb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_oof = np.zeros((len(CFG['k']), len(CFG['seed_l']), train.shape[0], CFG['n_class']))\n",
    "gbt_oof = np.zeros((len(CFG['k']), len(CFG['seed_l']), train.shape[0], CFG['n_class']))\n",
    "nn_pred = np.zeros((len(CFG['k']), len(CFG['seed_l']), test.shape[0], CFG['n_class']))\n",
    "gbt_pred = np.zeros((len(CFG['k']), len(CFG['seed_l']), test.shape[0], CFG['n_class']))\n",
    "eval_fold_result = {}\n",
    "\n",
    "for n_model in range(len(CFG['k'])):\n",
    "    print(f\"===== MODEL {n_model} cross validation =====\")\n",
    "\n",
    "    for n_seed, seed in enumerate(CFG['seed_l']):\n",
    "    \n",
    "        skf = StratifiedKFold(n_splits=CFG['n_splits'], shuffle=True, random_state=seed)\n",
    "\n",
    "        for fold, (trn_idx, val_idx) in enumerate(skf.split(train, train[CFG['target']])):\n",
    "            X_train, y_train_ohe, y_train = train[features].iloc[trn_idx], target_ohe.iloc[trn_idx], target.iloc[trn_idx]\n",
    "            X_valid, y_valid_ohe, y_valid = train[features].iloc[val_idx], target_ohe.iloc[val_idx], target.iloc[val_idx]\n",
    "            X_test = test[features]\n",
    "\n",
    "            X_train_2d, X_valid_2d = train_2d[trn_idx], train_2d[val_idx]\n",
    "            X_test_2d = test_2d\n",
    "            \n",
    "            X_train_knn, X_valid_knn = train_knn[n_model][trn_idx], train_knn[n_model][val_idx]\n",
    "            X_test_knn = test_knn[n_model]\n",
    "\n",
    "            X_train_km, X_valid_km = train_km[n_model][trn_idx], train_km[n_model][val_idx]\n",
    "            X_test_km = test_km[n_model]\n",
    "\n",
    "            X_train_pca, X_valid_pca = train_pca[n_model][trn_idx], train_pca[n_model][val_idx]\n",
    "            X_test_pca = test_pca[n_model]\n",
    "\n",
    "            log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            tb_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "            K.clear_session()  \n",
    "            shape = [\n",
    "                X_train.shape[1],\n",
    "                X_train_2d.shape[1:],\n",
    "                CFG['k'][n_model]*CFG['n_class'],\n",
    "                CFG['n_clusters'][n_model],\n",
    "                CFG['n_components'][n_model]\n",
    "            ]\n",
    "\n",
    "            model, hidden_model = create_model(shape, emb_dims, CFG['sigma'], CFG['n_conv2d'], n_model)\n",
    "            model.fit(\n",
    "                [X_train, X_train_2d, X_train_knn, X_train_km, X_train_pca], y_train_ohe,\n",
    "                batch_size=CFG['batch_size'],\n",
    "                epochs=CFG['max_epochs'],\n",
    "                validation_data=([X_valid, X_valid_2d, X_valid_knn, X_valid_km, X_valid_pca], y_valid_ohe),\n",
    "                callbacks=[es_cb, sch_cb, tb_cb],\n",
    "                verbose=CFG['nn_verbose']\n",
    "            )\n",
    "\n",
    "            nn_oof[n_model, n_seed, val_idx] = model.predict([X_valid, X_valid_2d, X_valid_knn, X_valid_km, X_valid_pca])\n",
    "            nn_pred[n_model, n_seed] += model.predict([X_test, X_test_2d, X_test_knn, X_test_km, X_test_pca]) / CFG['n_splits']\n",
    "            m_logloss = log_loss(y_valid_ohe, nn_oof[n_model, n_seed, val_idx])\n",
    "            print(f\"nn  model{n_model} seed{seed} fold{fold}: m_logloss {m_logloss}\")\n",
    "            \n",
    "            hidden_train = hidden_model.predict([X_train, X_train_2d, X_train_knn, X_train_km, X_train_pca])\n",
    "            hidden_valid = hidden_model.predict([X_valid, X_valid_2d, X_valid_knn, X_valid_km, X_valid_pca])\n",
    "            hidden_test = hidden_model.predict([X_test, X_test_2d, X_test_knn, X_test_km, X_test_pca])\n",
    "            trn_data = xgb.DMatrix(data=hidden_train, label=y_train)\n",
    "            val_data = xgb.DMatrix(data=hidden_valid, label=y_valid)\n",
    "\n",
    "            xgb_params['seed'] = seed\n",
    "            gbt_model = xgb.train(\n",
    "                params=xgb_params,\n",
    "                dtrain=trn_data,\n",
    "                evals=[(trn_data, \"train\"), (val_data, \"valid\")],\n",
    "                evals_result=eval_fold_result,\n",
    "                num_boost_round = CFG['n_estimators'],\n",
    "                verbose_eval=CFG['gbt_verbose'],\n",
    "                early_stopping_rounds=CFG['early_stopping_rounds'],\n",
    "                )\n",
    "        \n",
    "            gbt_oof[n_model, n_seed, val_idx] = gbt_model.predict(xgb.DMatrix(hidden_valid), ntree_limit=gbt_model.best_ntree_limit)\n",
    "            gbt_pred[n_model, n_seed] += gbt_model.predict(xgb.DMatrix(hidden_test), ntree_limit=gbt_model.best_ntree_limit) / CFG['n_splits']\n",
    "            m_logloss = log_loss(y_valid, gbt_oof[n_model, n_seed, val_idx])\n",
    "            print(f\"xgb model{n_model} seed{seed} fold{fold}: m_logloss {m_logloss}\")\n",
    "        \n",
    "        print(\"-\"*60)\n",
    "        m_logloss = log_loss(target, nn_oof[n_model, n_seed])\n",
    "        print(f\"nn  model{n_model} seed{seed}: m_logloss {m_logloss}\")\n",
    "        m_logloss = log_loss(target, gbt_oof[n_model, n_seed])\n",
    "        print(f\"xgb model{n_model} seed{seed}: m_logloss {m_logloss}\\n\")\n",
    "\n",
    "        np.save(CFG['save_path'] + f\"nn_model{n_model}_seed{seed}_oof\", nn_oof[n_model, n_seed])\n",
    "        np.save(CFG['save_path'] + f\"nn_model{n_model}_seed{seed}_pred\", nn_pred[n_model, n_seed])\n",
    "        np.save(CFG['save_path'] + f\"xgb_model{n_model}_seed{seed}_oof\", gbt_oof[n_model, n_seed])\n",
    "        np.save(CFG['save_path'] + f\"xgb_model{n_model}_seed{seed}_pred\", gbt_pred[n_model, n_seed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3246acb",
   "metadata": {},
   "source": [
    "## Post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421298da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_optimizer(X, a0, a1, a2, a3, a4, a5, a6, a7, a8):\n",
    "    oof = np.array([X[0]*a0, X[1]*a1, X[2]*a2, X[3]*a3, X[4]*a4, X[5]*a5, X[6]*a6, X[7]*a7, X[8]*a8]).transpose()\n",
    "    oof = oof / np.sum(oof, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    return log_loss(target, oof)\n",
    "\n",
    "def get_optimized(X, vals):\n",
    "    opt_val = 0\n",
    "    for i, val in enumerate(vals):\n",
    "        if i != len(X):\n",
    "            opt_val += X[i]*val\n",
    "        else:\n",
    "            coef = 1\n",
    "            for j in range(i):\n",
    "                coef -= X[j]\n",
    "            opt_val += coef*val\n",
    "\n",
    "    return opt_val\n",
    "\n",
    "def model_optimizer(X, oofs):\n",
    "    opt_oof = get_optimized(X, oofs)\n",
    "    \n",
    "    return log_loss(target, opt_oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e37145",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = np.concatenate([nn_oof, gbt_oof])\n",
    "pred = np.concatenate([nn_pred, gbt_pred])\n",
    "\n",
    "res_l = []\n",
    "for idx0 in tqdm(range(oof.shape[0])):\n",
    "    for idx1 in tqdm(range(oof.shape[1]), leave=False):\n",
    "        res = minimize(\n",
    "            fun=class_optimizer,\n",
    "            x0=[1.0 for _ in range(CFG['n_class'])],\n",
    "            args=tuple(oof[idx0, idx1, :, i] for i in range(CFG['n_class'])),\n",
    "            method='Nelder-Mead',\n",
    "            options={'maxiter': 300})\n",
    "\n",
    "        oof[idx0, idx1] = np.array([res.x[i]*oof[idx0, idx1, :, i] for i in range(CFG['n_class'])]).transpose()\n",
    "        oof[idx0, idx1] = oof[idx0, idx1] / np.sum(oof[idx0, idx1], axis=1).reshape(-1, 1)\n",
    "\n",
    "        pred[idx0, idx1] = np.array([res.x[i]*pred[idx0, idx1, :, i] for i in range(CFG['n_class'])]).transpose()\n",
    "        pred[idx0, idx1] = pred[idx0, idx1] / np.sum(pred[idx0, idx1], axis=1).reshape(-1, 1)\n",
    "\n",
    "        res_l.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20051eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_oof = np.mean(np.array(oof), axis=1)\n",
    "avg_pred = np.mean(np.array(pred), axis=1)\n",
    "\n",
    "res = minimize(\n",
    "    fun=model_optimizer,\n",
    "    x0=[1/oof.shape[0] for _ in range(avg_oof.shape[0]-1)],\n",
    "    args=tuple([avg_oof]),\n",
    "    method='Nelder-Mead',\n",
    "    options={'maxiter': 1000})\n",
    "\n",
    "opt_oof = get_optimized(res.x, avg_oof)\n",
    "opt_pred = get_optimized(res.x, avg_pred)\n",
    "\n",
    "print(f\"logloss score: {log_loss(target, opt_oof)}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0c6fb",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3929236e",
   "metadata": {},
   "source": [
    "### Target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c97894",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4), tight_layout=True)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "target.hist()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "pd.Series(opt_oof.argmax(axis=1)).hist()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "pd.Series(opt_pred.argmax(axis=1)).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244a3d9",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(target, opt_oof.argmax(axis=1))\n",
    "\n",
    "plt.figure(figsize=((16,4)))\n",
    "sns.heatmap(cm, annot=True, fmt='5d', cmap='Blues')\n",
    "plt.savefig(\"confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb17f5c",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(target, opt_oof.argmax(axis=1), digits=4))\n",
    "\n",
    "report = pd.DataFrame(classification_report(target, opt_oof.argmax(axis=1), digits=4, output_dict=True)).transpose()\n",
    "report.to_csv(\"report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a16abe",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9adf99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:, 1:] = opt_pred  \n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8), tight_layout=True)\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.title(f\"Class_{i+1}\")\n",
    "    submission[f'Class_{i+1}'].hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
