{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd40650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.9/site-packages (1.4.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from xgboost) (1.6.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from xgboost) (1.20.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "306a0ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap-hypetune\n",
      "  Downloading shap_hypetune-0.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from shap-hypetune) (1.6.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from shap-hypetune) (1.20.3)\n",
      "Collecting shap>=0.39.0\n",
      "  Downloading shap-0.39.0.tar.gz (356 kB)\n",
      "\u001b[K     |████████████████████████████████| 356 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from shap>=0.39.0->shap-hypetune) (0.24.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from shap>=0.39.0->shap-hypetune) (1.2.4)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /opt/conda/lib/python3.9/site-packages (from shap>=0.39.0->shap-hypetune) (4.61.1)\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.9/site-packages (from shap>=0.39.0->shap-hypetune) (0.53.1)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.9/site-packages (from shap>=0.39.0->shap-hypetune) (1.6.0)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.9/site-packages (from numba->shap>=0.39.0->shap-hypetune) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from numba->shap>=0.39.0->shap-hypetune) (49.6.0.post20210108)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->shap>=0.39.0->shap-hypetune) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->shap>=0.39.0->shap-hypetune) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->shap>=0.39.0->shap-hypetune) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->shap>=0.39.0->shap-hypetune) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->shap>=0.39.0->shap-hypetune) (1.0.1)\n",
      "Building wheels for collected packages: shap\n",
      "  Building wheel for shap (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for shap: filename=shap-0.39.0-cp39-cp39-linux_x86_64.whl size=418995 sha256=a6abefb588c34be1720d31a71786852b07e7274306af83cc1dd7a03201b6d4a1\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/bb/91/16/f6a057925f93af7e4281f6afce3495b595b473342766eb451c\n",
      "Successfully built shap\n",
      "Installing collected packages: slicer, shap, shap-hypetune\n",
      "Successfully installed shap-0.39.0 shap-hypetune-0.1.0 slicer-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install shap-hypetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "471fd3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(777)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "008cd520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0   0          0          0          6          1          0          0   \n",
       "1   1          0          0          0          0          0          0   \n",
       "2   2          0          0          0          0          0          1   \n",
       "3   3          0          0          7          0          1          5   \n",
       "4   4          1          0          0          0          0          0   \n",
       "\n",
       "   feature_6  feature_7  feature_8  ...  feature_66  feature_67  feature_68  \\\n",
       "0          0          0          7  ...           0           0           0   \n",
       "1          0          0          0  ...           2           0           0   \n",
       "2          0          3          0  ...           0           0           0   \n",
       "3          2          2          0  ...           0           4           0   \n",
       "4          0          0          0  ...           0           0           0   \n",
       "\n",
       "   feature_69  feature_70  feature_71  feature_72  feature_73  feature_74  \\\n",
       "0           0           0           0           2           0           0   \n",
       "1           0           0           0           0           1           0   \n",
       "2           0           1           0           0           0           0   \n",
       "3           2           2           0           4           3           0   \n",
       "4           0           0           0           0           0           0   \n",
       "\n",
       "    target  \n",
       "0  Class_6  \n",
       "1  Class_6  \n",
       "2  Class_2  \n",
       "3  Class_8  \n",
       "4  Class_2  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "259009fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0  200000          0          0          0          0          0          0   \n",
       "1  200001          1          2          0          0          0          0   \n",
       "2  200002          0          1          7          1          0          0   \n",
       "3  200003          0          0          0          4          3          1   \n",
       "4  200004          0          0          5          0          0          0   \n",
       "\n",
       "   feature_6  feature_7  feature_8  ...  feature_65  feature_66  feature_67  \\\n",
       "0          0          0          0  ...           0           0           0   \n",
       "1          0          0          0  ...           3           1           3   \n",
       "2          0          0          6  ...           3           0           0   \n",
       "3          0          0          0  ...           0           0           0   \n",
       "4          0          0          0  ...           0           0           0   \n",
       "\n",
       "   feature_68  feature_69  feature_70  feature_71  feature_72  feature_73  \\\n",
       "0           0           0           0           0           0           0   \n",
       "1           0           0           0           0           3           0   \n",
       "2           0           0           3           0           2           0   \n",
       "3           1           0           0           0           4           0   \n",
       "4           0           0           0           0           0           1   \n",
       "\n",
       "   feature_74  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d2ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = pd.DataFrame(columns = ['percent', 'depth', 'missing_features', 'log_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aed7cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target', ylabel='count'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=\"target\", data=train_df, order=train_df['target'].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1fc6da1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The train_size = 1 should be greater or equal to the number of classes = 9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-fe0ff4e5315f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_percent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_no_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_percent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train count: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test count: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2195\u001b[0m                      random_state=random_state)\n\u001b[1;32m   2196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2197\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m     return list(chain.from_iterable((_safe_indexing(a, train),\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \"\"\"\n\u001b[1;32m   1386\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1387\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1388\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1721\u001b[0;31m             raise ValueError('The train_size = %d should be greater or '\n\u001b[0m\u001b[1;32m   1722\u001b[0m                              \u001b[0;34m'equal to the number of classes = %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m                              (n_train, n_classes))\n",
      "\u001b[0;31mValueError\u001b[0m: The train_size = 1 should be greater or equal to the number of classes = 9"
     ]
    }
   ],
   "source": [
    "train_no_predict = train_df.drop(['id', 'target'], 1)\n",
    "train_predict = train_df['target']\n",
    "\n",
    "train_percent = 0.80\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_no_predict, train_predict, stratify=train_predict, train_size=train_percent)\n",
    "print('train count: ', len(y_train))\n",
    "print('test count: ', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3cd916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:49:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:51:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:53:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:55:27] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:57:35] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:59:49] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:03:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:06:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:08:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:10:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:12:17] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:14:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:16:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:18:30] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:20:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:22:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:24:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:26:39] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:28:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:30:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:32:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:40] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:36:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:38:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:43:00] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:45:10] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:47:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:49:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:51:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:53:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:55:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:57:46] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:59:48] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:01:55] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:04:11] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:06:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:08:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:10:35] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:12:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:14:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:16:53] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:19:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:21:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:23:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:25:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:26:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:28:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:30:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:32:43] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:34:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:42:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:46:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:48:06] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:49:49] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:51:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:53:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:55:10] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:56:54] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:58:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:00:13] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:01:53] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:03:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:04:59] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:06:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:07:46] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:09:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:10:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:11:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:11:40] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<shaphypetune.BoostRFE>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shaphypetune import BoostRFE\n",
    "\n",
    "model = BoostRFE(XGBClassifier(), \n",
    "                 min_features_to_select=1, step=1,\n",
    "                 importance_type='shap_importances', train_importance=False)\n",
    "\n",
    "model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=6, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f4fe940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "print(model.estimator_)\n",
    "print(model.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "860a6dc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  1  1  1  1  1  9 14  1  1  1  2  1  1  1  5  1  1  1  1  1  1 12  1\n",
      "  4  1  1  6  1  1  1  1  1  1  1  3 11  1  1  1  1 13  8  1  1  1  1 15\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1 10]\n",
      "to_use [['feature_1']\n",
      " ['feature_2']\n",
      " ['feature_3']\n",
      " ['feature_4']\n",
      " ['feature_5']\n",
      " ['feature_8']\n",
      " ['feature_9']\n",
      " ['feature_10']\n",
      " ['feature_12']\n",
      " ['feature_13']\n",
      " ['feature_14']\n",
      " ['feature_16']\n",
      " ['feature_17']\n",
      " ['feature_18']\n",
      " ['feature_19']\n",
      " ['feature_20']\n",
      " ['feature_21']\n",
      " ['feature_23']\n",
      " ['feature_25']\n",
      " ['feature_26']\n",
      " ['feature_28']\n",
      " ['feature_29']\n",
      " ['feature_30']\n",
      " ['feature_31']\n",
      " ['feature_32']\n",
      " ['feature_33']\n",
      " ['feature_34']\n",
      " ['feature_37']\n",
      " ['feature_38']\n",
      " ['feature_39']\n",
      " ['feature_40']\n",
      " ['feature_43']\n",
      " ['feature_44']\n",
      " ['feature_45']\n",
      " ['feature_46']\n",
      " ['feature_48']\n",
      " ['feature_49']\n",
      " ['feature_50']\n",
      " ['feature_51']\n",
      " ['feature_52']\n",
      " ['feature_53']\n",
      " ['feature_54']\n",
      " ['feature_55']\n",
      " ['feature_56']\n",
      " ['feature_57']\n",
      " ['feature_58']\n",
      " ['feature_59']\n",
      " ['feature_60']\n",
      " ['feature_61']\n",
      " ['feature_62']\n",
      " ['feature_63']\n",
      " ['feature_64']\n",
      " ['feature_65']\n",
      " ['feature_66']\n",
      " ['feature_67']\n",
      " ['feature_68']\n",
      " ['feature_69']\n",
      " ['feature_70']\n",
      " ['feature_71']\n",
      " ['feature_72']\n",
      " ['feature_73']]\n",
      "to_avoid [['feature_0']\n",
      " ['feature_6']\n",
      " ['feature_7']\n",
      " ['feature_11']\n",
      " ['feature_15']\n",
      " ['feature_22']\n",
      " ['feature_24']\n",
      " ['feature_27']\n",
      " ['feature_35']\n",
      " ['feature_36']\n",
      " ['feature_41']\n",
      " ['feature_42']\n",
      " ['feature_47']\n",
      " ['feature_74']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n"
     ]
    }
   ],
   "source": [
    "dir(model)\n",
    "print(model.ranking_)\n",
    "dir(model.estimator_)\n",
    "#model.ranking_\n",
    "model.estimator_.evals_result_\n",
    "\n",
    "good_feature_indexes = np.argwhere(model.ranking_ <= 1)\n",
    "features_to_use = X_train.columns[good_feature_indexes]\n",
    "print('to_use', features_to_use)\n",
    "\n",
    "\n",
    "bad_feature_indexes = np.argwhere(model.ranking_ > 1)\n",
    "features_to_avoid = X_train.columns[bad_feature_indexes]\n",
    "print('to_avoid', features_to_avoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bad092eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_0 : 7\n",
      "feature_1 : 1\n",
      "feature_2 : 1\n",
      "feature_3 : 1\n",
      "feature_4 : 1\n",
      "feature_5 : 1\n",
      "feature_6 : 9\n",
      "feature_7 : 14\n",
      "feature_8 : 1\n",
      "feature_9 : 1\n",
      "feature_10 : 1\n",
      "feature_11 : 2\n",
      "feature_12 : 1\n",
      "feature_13 : 1\n",
      "feature_14 : 1\n",
      "feature_15 : 5\n",
      "feature_16 : 1\n",
      "feature_17 : 1\n",
      "feature_18 : 1\n",
      "feature_19 : 1\n",
      "feature_20 : 1\n",
      "feature_21 : 1\n",
      "feature_22 : 12\n",
      "feature_23 : 1\n",
      "feature_24 : 4\n",
      "feature_25 : 1\n",
      "feature_26 : 1\n",
      "feature_27 : 6\n",
      "feature_28 : 1\n",
      "feature_29 : 1\n",
      "feature_30 : 1\n",
      "feature_31 : 1\n",
      "feature_32 : 1\n",
      "feature_33 : 1\n",
      "feature_34 : 1\n",
      "feature_35 : 3\n",
      "feature_36 : 11\n",
      "feature_37 : 1\n",
      "feature_38 : 1\n",
      "feature_39 : 1\n",
      "feature_40 : 1\n",
      "feature_41 : 13\n",
      "feature_42 : 8\n",
      "feature_43 : 1\n",
      "feature_44 : 1\n",
      "feature_45 : 1\n",
      "feature_46 : 1\n",
      "feature_47 : 15\n",
      "feature_48 : 1\n",
      "feature_49 : 1\n",
      "feature_50 : 1\n",
      "feature_51 : 1\n",
      "feature_52 : 1\n",
      "feature_53 : 1\n",
      "feature_54 : 1\n",
      "feature_55 : 1\n",
      "feature_56 : 1\n",
      "feature_57 : 1\n",
      "feature_58 : 1\n",
      "feature_59 : 1\n",
      "feature_60 : 1\n",
      "feature_61 : 1\n",
      "feature_62 : 1\n",
      "feature_63 : 1\n",
      "feature_64 : 1\n",
      "feature_65 : 1\n",
      "feature_66 : 1\n",
      "feature_67 : 1\n",
      "feature_68 : 1\n",
      "feature_69 : 1\n",
      "feature_70 : 1\n",
      "feature_71 : 1\n",
      "feature_72 : 1\n",
      "feature_73 : 1\n",
      "feature_74 : 10\n"
     ]
    }
   ],
   "source": [
    "for i, rank in enumerate(model.ranking_):\n",
    "    feature = X_train.columns[i]\n",
    "    print(feature, ':', rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "af541e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-20 13:41:50.857265\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:02:10] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"early_stopping_rounds\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:02:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2021-06-20 15:09:17.903697\n"
     ]
    }
   ],
   "source": [
    "import datetime;\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "missing_columns = ['feature_47', 'feature_7']\n",
    "\n",
    "X_train_without_worst = X_train.drop(missing_columns, 1)\n",
    "X_test_without_worst = X_test.drop(missing_columns, 1)\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "parameters = { 'seed': [777], 'n_estimators': [300], 'early_stopping_rounds': [25],\n",
    "               'learning_rate': [0.05], 'max_depth': [5], 'subsample': [0.8] }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters, \n",
    "                   cv=StratifiedKFold(n_splits=6, shuffle=True),\n",
    "                   n_jobs=-1, verbose=2, refit=True, scoring='neg_log_loss')\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "clf.fit(X_train_without_worst, y_train)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4f7f22d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'early_stopping_rounds': 25, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 300, 'seed': 777, 'subsample': 0.8}\n",
      "test log_loss -1.7510347804718587\n",
      "1.7496554059509188\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>early_stopping_rounds</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>seed</th>\n",
       "      <th>subsample</th>\n",
       "      <th>missing_columns</th>\n",
       "      <th>train_percent</th>\n",
       "      <th>log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7, feature_41</td>\n",
       "      <td>10.0%</td>\n",
       "      <td>1.765198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7, feature_41</td>\n",
       "      <td>10.0%</td>\n",
       "      <td>1.767728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>1.750769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>1.750218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>1.749678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>1.749346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>1.749655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  early_stopping_rounds  learning_rate max_depth n_estimators seed  subsample  \\\n",
       "0                    25           0.05         4          100  777        0.8   \n",
       "1                    25           0.05         3          100  777        0.8   \n",
       "2                    25           0.05         3          250  777        0.8   \n",
       "3                    25           0.05         4          500  777        0.8   \n",
       "4                    25           0.05         3          500  777        0.8   \n",
       "5                    25           0.05         4          300  777        0.8   \n",
       "6                    25           0.05         5          300  777        0.8   \n",
       "\n",
       "                     missing_columns train_percent  log_loss  \n",
       "0  feature_47, feature_7, feature_41         10.0%  1.765198  \n",
       "1  feature_47, feature_7, feature_41         10.0%  1.767728  \n",
       "2              feature_47, feature_7         80.0%  1.750769  \n",
       "3              feature_47, feature_7         80.0%  1.750218  \n",
       "4              feature_47, feature_7         80.0%  1.749678  \n",
       "5              feature_47, feature_7         80.0%  1.749346  \n",
       "6              feature_47, feature_7         80.0%  1.749655  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf.best_params_)\n",
    "print('test log_loss', clf.best_score_)\n",
    "\n",
    "# run on the holdout data\n",
    "test_probs = clf.predict_proba(X_test_without_worst)\n",
    "the_log_loss = abs(log_loss(y_test, test_probs))\n",
    "print(the_log_loss)\n",
    "\n",
    "params = clf.best_params_.copy()\n",
    "params['missing_columns'] = ', '.join(map(str, missing_columns))\n",
    "params['train_percent'] = str(train_percent * 100) + '%'\n",
    "params['log_loss'] = the_log_loss\n",
    "\n",
    "#results_df = pd.DataFrame(columns = params)\n",
    "results_df = results_df.append(params, ignore_index=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "913f1353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df = results_df.drop(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dad4bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7496554059509188\n",
      "1.7496554059509188\n"
     ]
    }
   ],
   "source": [
    "the_log_loss = abs(log_loss(y_test, test_probs))\n",
    "the_clip = np.clip(test_probs, 0.001, 0.999)\n",
    "the_clip_log_loss = abs(log_loss(y_test, the_clip))\n",
    "\n",
    "print(the_log_loss)\n",
    "print(the_clip_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e1bd95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_modified = test_df.drop('id', 1)\n",
    "test_df_modified = test_df_modified.drop(missing_columns, 1)\n",
    "test_preds = clf.predict_proba(test_df_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "006ffc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(test_preds)\n",
    "submission.columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9']\n",
    "submission['id'] = test_df['id']\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
