{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd40650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.9/site-packages (1.4.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from xgboost) (1.6.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from xgboost) (1.20.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "306a0ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap-hypetune\n",
      "  Downloading shap_hypetune-0.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from shap-hypetune) (1.6.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from shap-hypetune) (1.20.3)\n",
      "Collecting shap>=0.39.0\n",
      "  Downloading shap-0.39.0.tar.gz (356 kB)\n",
      "\u001b[K     |████████████████████████████████| 356 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from shap>=0.39.0->shap-hypetune) (0.24.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from shap>=0.39.0->shap-hypetune) (1.2.4)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /opt/conda/lib/python3.9/site-packages (from shap>=0.39.0->shap-hypetune) (4.61.1)\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.9/site-packages (from shap>=0.39.0->shap-hypetune) (0.53.1)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.9/site-packages (from shap>=0.39.0->shap-hypetune) (1.6.0)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.9/site-packages (from numba->shap>=0.39.0->shap-hypetune) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from numba->shap>=0.39.0->shap-hypetune) (49.6.0.post20210108)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->shap>=0.39.0->shap-hypetune) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->shap>=0.39.0->shap-hypetune) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->shap>=0.39.0->shap-hypetune) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->shap>=0.39.0->shap-hypetune) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->shap>=0.39.0->shap-hypetune) (1.0.1)\n",
      "Building wheels for collected packages: shap\n",
      "  Building wheel for shap (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for shap: filename=shap-0.39.0-cp39-cp39-linux_x86_64.whl size=418995 sha256=a6abefb588c34be1720d31a71786852b07e7274306af83cc1dd7a03201b6d4a1\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/bb/91/16/f6a057925f93af7e4281f6afce3495b595b473342766eb451c\n",
      "Successfully built shap\n",
      "Installing collected packages: slicer, shap, shap-hypetune\n",
      "Successfully installed shap-0.39.0 shap-hypetune-0.1.0 slicer-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install shap-hypetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "471fd3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(777)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "008cd520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0   0          0          0          6          1          0          0   \n",
       "1   1          0          0          0          0          0          0   \n",
       "2   2          0          0          0          0          0          1   \n",
       "3   3          0          0          7          0          1          5   \n",
       "4   4          1          0          0          0          0          0   \n",
       "\n",
       "   feature_6  feature_7  feature_8  ...  feature_66  feature_67  feature_68  \\\n",
       "0          0          0          7  ...           0           0           0   \n",
       "1          0          0          0  ...           2           0           0   \n",
       "2          0          3          0  ...           0           0           0   \n",
       "3          2          2          0  ...           0           4           0   \n",
       "4          0          0          0  ...           0           0           0   \n",
       "\n",
       "   feature_69  feature_70  feature_71  feature_72  feature_73  feature_74  \\\n",
       "0           0           0           0           2           0           0   \n",
       "1           0           0           0           0           1           0   \n",
       "2           0           1           0           0           0           0   \n",
       "3           2           2           0           4           3           0   \n",
       "4           0           0           0           0           0           0   \n",
       "\n",
       "    target  \n",
       "0  Class_6  \n",
       "1  Class_6  \n",
       "2  Class_2  \n",
       "3  Class_8  \n",
       "4  Class_2  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "df685102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5\n",
      "1    5\n",
      "2    1\n",
      "3    7\n",
      "4    1\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "features_target = ['target'] \n",
    "\n",
    "for feature in features_target:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_df[feature])\n",
    "    train_df[feature] = le.transform(train_df[feature])\n",
    "    \n",
    "print(train_df['target'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "259009fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0  200000          0          0          0          0          0          0   \n",
       "1  200001          1          2          0          0          0          0   \n",
       "2  200002          0          1          7          1          0          0   \n",
       "3  200003          0          0          0          4          3          1   \n",
       "4  200004          0          0          5          0          0          0   \n",
       "\n",
       "   feature_6  feature_7  feature_8  ...  feature_65  feature_66  feature_67  \\\n",
       "0          0          0          0  ...           0           0           0   \n",
       "1          0          0          0  ...           3           1           3   \n",
       "2          0          0          6  ...           3           0           0   \n",
       "3          0          0          0  ...           0           0           0   \n",
       "4          0          0          0  ...           0           0           0   \n",
       "\n",
       "   feature_68  feature_69  feature_70  feature_71  feature_72  feature_73  \\\n",
       "0           0           0           0           0           0           0   \n",
       "1           0           0           0           0           3           0   \n",
       "2           0           0           3           0           2           0   \n",
       "3           1           0           0           0           4           0   \n",
       "4           0           0           0           0           0           1   \n",
       "\n",
       "   feature_74  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f3d2ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = pd.DataFrame(columns = ['percent', 'depth', 'missing_features', 'log_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0aed7cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target', ylabel='count'>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=\"target\", data=train_df, order=train_df['target'].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1fc6da1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train count:  160000\n",
      "test count:  40000\n"
     ]
    }
   ],
   "source": [
    "train_no_predict = train_df.drop(['id', 'target'], 1)\n",
    "train_predict = train_df['target']\n",
    "\n",
    "train_percent = 0.80\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_no_predict, train_predict, stratify=train_predict, train_size=train_percent)\n",
    "print('train count: ', len(y_train))\n",
    "print('test count: ', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3cd916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:49:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:51:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:53:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:55:27] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:57:35] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:59:49] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:03:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:06:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:08:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:10:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:12:17] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:14:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:16:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:18:30] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:20:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:22:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:24:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:26:39] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:28:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:30:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:32:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:40] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:36:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:38:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:43:00] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:45:10] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:47:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:49:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:51:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:53:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:55:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:57:46] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:59:48] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:01:55] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:04:11] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:06:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:08:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:10:35] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:12:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:14:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:16:53] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:19:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:21:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:23:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:25:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:26:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:28:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:30:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:32:43] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:34:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:36:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:42:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:46:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:48:06] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:49:49] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:51:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:53:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:55:10] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:56:54] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:58:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:00:13] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:01:53] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:03:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:04:59] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:06:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:07:46] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:09:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:10:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:11:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:11:40] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<shaphypetune.BoostRFE>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shaphypetune import BoostRFE\n",
    "\n",
    "model = BoostRFE(XGBClassifier(), \n",
    "                 min_features_to_select=1, step=1,\n",
    "                 importance_type='shap_importances', train_importance=False)\n",
    "\n",
    "model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=6, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f4fe940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "print(model.estimator_)\n",
    "print(model.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "860a6dc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  1  1  1  1  1  9 14  1  1  1  2  1  1  1  5  1  1  1  1  1  1 12  1\n",
      "  4  1  1  6  1  1  1  1  1  1  1  3 11  1  1  1  1 13  8  1  1  1  1 15\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1 10]\n",
      "to_use [['feature_1']\n",
      " ['feature_2']\n",
      " ['feature_3']\n",
      " ['feature_4']\n",
      " ['feature_5']\n",
      " ['feature_8']\n",
      " ['feature_9']\n",
      " ['feature_10']\n",
      " ['feature_12']\n",
      " ['feature_13']\n",
      " ['feature_14']\n",
      " ['feature_16']\n",
      " ['feature_17']\n",
      " ['feature_18']\n",
      " ['feature_19']\n",
      " ['feature_20']\n",
      " ['feature_21']\n",
      " ['feature_23']\n",
      " ['feature_25']\n",
      " ['feature_26']\n",
      " ['feature_28']\n",
      " ['feature_29']\n",
      " ['feature_30']\n",
      " ['feature_31']\n",
      " ['feature_32']\n",
      " ['feature_33']\n",
      " ['feature_34']\n",
      " ['feature_37']\n",
      " ['feature_38']\n",
      " ['feature_39']\n",
      " ['feature_40']\n",
      " ['feature_43']\n",
      " ['feature_44']\n",
      " ['feature_45']\n",
      " ['feature_46']\n",
      " ['feature_48']\n",
      " ['feature_49']\n",
      " ['feature_50']\n",
      " ['feature_51']\n",
      " ['feature_52']\n",
      " ['feature_53']\n",
      " ['feature_54']\n",
      " ['feature_55']\n",
      " ['feature_56']\n",
      " ['feature_57']\n",
      " ['feature_58']\n",
      " ['feature_59']\n",
      " ['feature_60']\n",
      " ['feature_61']\n",
      " ['feature_62']\n",
      " ['feature_63']\n",
      " ['feature_64']\n",
      " ['feature_65']\n",
      " ['feature_66']\n",
      " ['feature_67']\n",
      " ['feature_68']\n",
      " ['feature_69']\n",
      " ['feature_70']\n",
      " ['feature_71']\n",
      " ['feature_72']\n",
      " ['feature_73']]\n",
      "to_avoid [['feature_0']\n",
      " ['feature_6']\n",
      " ['feature_7']\n",
      " ['feature_11']\n",
      " ['feature_15']\n",
      " ['feature_22']\n",
      " ['feature_24']\n",
      " ['feature_27']\n",
      " ['feature_35']\n",
      " ['feature_36']\n",
      " ['feature_41']\n",
      " ['feature_42']\n",
      " ['feature_47']\n",
      " ['feature_74']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n"
     ]
    }
   ],
   "source": [
    "dir(model)\n",
    "print(model.ranking_)\n",
    "dir(model.estimator_)\n",
    "#model.ranking_\n",
    "model.estimator_.evals_result_\n",
    "\n",
    "good_feature_indexes = np.argwhere(model.ranking_ <= 1)\n",
    "features_to_use = X_train.columns[good_feature_indexes]\n",
    "print('to_use', features_to_use)\n",
    "\n",
    "\n",
    "bad_feature_indexes = np.argwhere(model.ranking_ > 1)\n",
    "features_to_avoid = X_train.columns[bad_feature_indexes]\n",
    "print('to_avoid', features_to_avoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bad092eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_0 : 7\n",
      "feature_1 : 1\n",
      "feature_2 : 1\n",
      "feature_3 : 1\n",
      "feature_4 : 1\n",
      "feature_5 : 1\n",
      "feature_6 : 9\n",
      "feature_7 : 14\n",
      "feature_8 : 1\n",
      "feature_9 : 1\n",
      "feature_10 : 1\n",
      "feature_11 : 2\n",
      "feature_12 : 1\n",
      "feature_13 : 1\n",
      "feature_14 : 1\n",
      "feature_15 : 5\n",
      "feature_16 : 1\n",
      "feature_17 : 1\n",
      "feature_18 : 1\n",
      "feature_19 : 1\n",
      "feature_20 : 1\n",
      "feature_21 : 1\n",
      "feature_22 : 12\n",
      "feature_23 : 1\n",
      "feature_24 : 4\n",
      "feature_25 : 1\n",
      "feature_26 : 1\n",
      "feature_27 : 6\n",
      "feature_28 : 1\n",
      "feature_29 : 1\n",
      "feature_30 : 1\n",
      "feature_31 : 1\n",
      "feature_32 : 1\n",
      "feature_33 : 1\n",
      "feature_34 : 1\n",
      "feature_35 : 3\n",
      "feature_36 : 11\n",
      "feature_37 : 1\n",
      "feature_38 : 1\n",
      "feature_39 : 1\n",
      "feature_40 : 1\n",
      "feature_41 : 13\n",
      "feature_42 : 8\n",
      "feature_43 : 1\n",
      "feature_44 : 1\n",
      "feature_45 : 1\n",
      "feature_46 : 1\n",
      "feature_47 : 15\n",
      "feature_48 : 1\n",
      "feature_49 : 1\n",
      "feature_50 : 1\n",
      "feature_51 : 1\n",
      "feature_52 : 1\n",
      "feature_53 : 1\n",
      "feature_54 : 1\n",
      "feature_55 : 1\n",
      "feature_56 : 1\n",
      "feature_57 : 1\n",
      "feature_58 : 1\n",
      "feature_59 : 1\n",
      "feature_60 : 1\n",
      "feature_61 : 1\n",
      "feature_62 : 1\n",
      "feature_63 : 1\n",
      "feature_64 : 1\n",
      "feature_65 : 1\n",
      "feature_66 : 1\n",
      "feature_67 : 1\n",
      "feature_68 : 1\n",
      "feature_69 : 1\n",
      "feature_70 : 1\n",
      "feature_71 : 1\n",
      "feature_72 : 1\n",
      "feature_73 : 1\n",
      "feature_74 : 10\n"
     ]
    }
   ],
   "source": [
    "for i, rank in enumerate(model.ranking_):\n",
    "    feature = X_train.columns[i]\n",
    "    print(feature, ':', rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d0e5dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce bottom 6\n",
    "from sklearn.decomposition import PCA\n",
    "pca_columns = ['feature_47', 'feature_7', 'feature_41', 'feature_22', 'feature_36', 'feature_74']\n",
    "pca_train_data = X_train[pca_columns]\n",
    "pca_test_data = X_test[pca_columns]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_train_results = pca.fit_transform(X = pca_train_data, y = y_train)\n",
    "pca_train_df = pd.DataFrame(data = pca_train_results, columns = ['pc1', 'pc2'])\n",
    "X_train_modifed = pd.concat([pca_train_df, X_train], axis = 1)\n",
    "\n",
    "pca_test_results = pca.transform(pca_test_data)\n",
    "pca_test_df = pd.DataFrame(data = pca_test_results, columns = ['pc1', 'pc2'])\n",
    "X_test_modifed = pd.concat([pca_test_df, X_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "06e86e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce next 6\n",
    "from sklearn.decomposition import PCA\n",
    "pca_columns2 = ['feature_6', 'feature_42', 'feature_0', 'feature_27', 'feature_15', 'feature_24']\n",
    "pca_train_data = X_train[pca_columns2]\n",
    "pca_test_data = X_test[pca_columns2]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_train_results = pca.fit_transform(X = pca_train_data, y = y_train)\n",
    "pca_train_df = pd.DataFrame(data = pca_train_results, columns = ['pc1.1', 'pc2.1'])\n",
    "X_train_modifed = pd.concat([pca_train_df, X_train], axis = 1)\n",
    "\n",
    "pca_test_results = pca.transform(pca_test_data)\n",
    "pca_test_df = pd.DataFrame(data = pca_test_results, columns = ['pc1.1', 'pc2.1'])\n",
    "X_test_modifed = pd.concat([pca_test_df, X_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "af541e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-20 21:48:24.020176\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:43:58] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"early_stopping_rounds\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:43:59] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2021-06-20 22:49:16.825585\n"
     ]
    }
   ],
   "source": [
    "import datetime;\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "missing_columns = np.concatenate([pca_columns])\n",
    "\n",
    "X_train_without_worst = X_train.drop(missing_columns, 1)\n",
    "X_test_without_worst = X_test.drop(missing_columns, 1)\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "parameters = { 'seed': [777], 'n_estimators': [300], 'early_stopping_rounds': [25],\n",
    "               'learning_rate': [0.05], 'max_depth': [4], 'subsample': [0.8] }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters, \n",
    "                   cv=StratifiedKFold(n_splits=6, shuffle=True),\n",
    "                   n_jobs=-1, verbose=2, refit=True, scoring='neg_log_loss')\n",
    "\n",
    "clf.fit(X_train_without_worst, y_train)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4f7f22d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'early_stopping_rounds': 25, 'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 300, 'seed': 777, 'subsample': 0.8}\n",
      "test log_loss -1.7511143539914589\n",
      "1.7514340982886962\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>early_stopping_rounds</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>seed</th>\n",
       "      <th>subsample</th>\n",
       "      <th>missing_columns</th>\n",
       "      <th>train_percent</th>\n",
       "      <th>log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7, feature_41</td>\n",
       "      <td>10.0%</td>\n",
       "      <td>1.765198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7, feature_41</td>\n",
       "      <td>10.0%</td>\n",
       "      <td>1.767728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>1.750769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>1.750218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>1.749678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>1.749346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>1.749655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>1.746734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7, feature_41, feature_22,...</td>\n",
       "      <td>66.0%</td>\n",
       "      <td>1.750781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7, feature_41, feature_22,...</td>\n",
       "      <td>66.0%</td>\n",
       "      <td>1.750926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>777</td>\n",
       "      <td>0.8</td>\n",
       "      <td>feature_47, feature_7, feature_41, feature_22,...</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>1.751434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   early_stopping_rounds  learning_rate max_depth n_estimators seed  \\\n",
       "0                     25           0.05         4          100  777   \n",
       "1                     25           0.05         3          100  777   \n",
       "2                     25           0.05         3          250  777   \n",
       "3                     25           0.05         4          500  777   \n",
       "4                     25           0.05         3          500  777   \n",
       "5                     25           0.05         4          300  777   \n",
       "6                     25           0.05         5          300  777   \n",
       "7                     25           0.05         4          300  777   \n",
       "8                     25           0.05         4          300  777   \n",
       "9                     25           0.05         4          300  777   \n",
       "10                    25           0.05         4          300  777   \n",
       "\n",
       "    subsample                                    missing_columns  \\\n",
       "0         0.8                  feature_47, feature_7, feature_41   \n",
       "1         0.8                  feature_47, feature_7, feature_41   \n",
       "2         0.8                              feature_47, feature_7   \n",
       "3         0.8                              feature_47, feature_7   \n",
       "4         0.8                              feature_47, feature_7   \n",
       "5         0.8                              feature_47, feature_7   \n",
       "6         0.8                              feature_47, feature_7   \n",
       "7         0.8                              feature_47, feature_7   \n",
       "8         0.8  feature_47, feature_7, feature_41, feature_22,...   \n",
       "9         0.8  feature_47, feature_7, feature_41, feature_22,...   \n",
       "10        0.8  feature_47, feature_7, feature_41, feature_22,...   \n",
       "\n",
       "   train_percent  log_loss  \n",
       "0          10.0%  1.765198  \n",
       "1          10.0%  1.767728  \n",
       "2          80.0%  1.750769  \n",
       "3          80.0%  1.750218  \n",
       "4          80.0%  1.749678  \n",
       "5          80.0%  1.749346  \n",
       "6          80.0%  1.749655  \n",
       "7          80.0%  1.746734  \n",
       "8          66.0%  1.750781  \n",
       "9          66.0%  1.750926  \n",
       "10         80.0%  1.751434  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf.best_params_)\n",
    "print('test log_loss', clf.best_score_)\n",
    "\n",
    "# run on the holdout data\n",
    "test_probs = clf.predict_proba(X_test_without_worst)\n",
    "the_log_loss = abs(log_loss(y_test, test_probs))\n",
    "print(the_log_loss)\n",
    "\n",
    "params = clf.best_params_.copy()\n",
    "params['missing_columns'] = ', '.join(map(str, missing_columns))\n",
    "params['train_percent'] = str(train_percent * 100) + '%'\n",
    "params['log_loss'] = the_log_loss\n",
    "\n",
    "#results_df = pd.DataFrame(columns = params)\n",
    "results_df = results_df.append(params, ignore_index=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4f979cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df = results_df.drop(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5dd07834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7496554059509188\n",
      "1.7496554059509188\n"
     ]
    }
   ],
   "source": [
    "the_log_loss = abs(log_loss(y_test, test_probs))\n",
    "the_clip = np.clip(test_probs, 0.001, 0.999)\n",
    "the_clip_log_loss = abs(log_loss(y_test, the_clip))\n",
    "\n",
    "print(the_log_loss)\n",
    "print(the_clip_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "945a949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_modified = test_df.drop('id', 1)\n",
    "test_df_modified = test_df_modified.drop(missing_columns, 1)\n",
    "test_preds = clf.predict_proba(test_df_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e4d76fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(test_preds)\n",
    "submission.columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9']\n",
    "submission['id'] = test_df['id']\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
