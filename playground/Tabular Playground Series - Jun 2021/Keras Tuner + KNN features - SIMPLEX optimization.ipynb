{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/tabular-playground-series-jun-2021/discussion/248846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d965431e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in /opt/conda/lib/python3.9/site-packages (1.0.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (20.9)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (2.5.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (1.19.5)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (7.24.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (2.25.1)\n",
      "Requirement already satisfied: kt-legacy in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (1.0.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (1.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (49.6.0.post20210108)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (0.18.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (3.0.19)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (5.0.9)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (0.1.2)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (2.9.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (5.0.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.9/site-packages (from jedi>=0.16->ipython->keras-tuner) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.9/site-packages (from pexpect>4.3->ipython->keras-tuner) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.9/site-packages (from traitlets>=4.2->ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->keras-tuner) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->keras-tuner) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->keras-tuner) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests->keras-tuner) (4.0.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (0.13.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (0.36.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (3.17.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (1.8.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (1.34.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (1.31.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from absl-py>=0.4->tensorboard->keras-tuner) (1.15.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e625c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations,callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from kerastuner import RandomSearch, BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d521176",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2021\n",
    "\n",
    "# OPTIM MODES\n",
    "# 1 - Only Keras Tuner\n",
    "# 2 - Keras Tuner + TOP models crossvalidation and submission\n",
    "# 3 - TOP models (NN configuration in dictionaty - params from local experiments) crossvalidation and submission\n",
    "\n",
    "OPTIM_MODE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de0074e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', index_col = 'id')\n",
    "test = pd.read_csv(\"test.csv\", index_col = 'id')\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "target = train.target\n",
    "targets = pd.get_dummies(train['target'])\n",
    "target_optim = train['target'].apply(lambda x: int(x.split(\"_\")[-1])-1)\n",
    "\n",
    "train_knn = np.load(\"add_feat_train.npy\")\n",
    "test_knn = np.load(\"add_feat_test.npy\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_knn = scaler.fit_transform(train_knn)\n",
    "test_knn = scaler.transform(test_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3b9a82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.105567</td>\n",
       "      <td>0.080686</td>\n",
       "      <td>0.108759</td>\n",
       "      <td>0.082797</td>\n",
       "      <td>0.081172</td>\n",
       "      <td>0.078104</td>\n",
       "      <td>0.080682</td>\n",
       "      <td>0.098372</td>\n",
       "      <td>0.089345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.029184</td>\n",
       "      <td>0.016157</td>\n",
       "      <td>0.020174</td>\n",
       "      <td>0.026179</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.016930</td>\n",
       "      <td>0.017309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.086819</td>\n",
       "      <td>0.070034</td>\n",
       "      <td>0.084438</td>\n",
       "      <td>0.069198</td>\n",
       "      <td>0.073911</td>\n",
       "      <td>0.082720</td>\n",
       "      <td>0.074481</td>\n",
       "      <td>0.085460</td>\n",
       "      <td>0.085939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.233975</td>\n",
       "      <td>0.222624</td>\n",
       "      <td>0.256220</td>\n",
       "      <td>0.227895</td>\n",
       "      <td>0.203315</td>\n",
       "      <td>0.250494</td>\n",
       "      <td>0.199114</td>\n",
       "      <td>0.231126</td>\n",
       "      <td>0.226965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.029336</td>\n",
       "      <td>0.019184</td>\n",
       "      <td>0.023764</td>\n",
       "      <td>0.021166</td>\n",
       "      <td>0.021726</td>\n",
       "      <td>0.027041</td>\n",
       "      <td>0.019323</td>\n",
       "      <td>0.031685</td>\n",
       "      <td>0.028023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.105567  0.080686  0.108759  0.082797  0.081172  0.078104  0.080682   \n",
       "1  0.029184  0.016157  0.020174  0.026179  0.023858  0.012390  0.027862   \n",
       "2  0.086819  0.070034  0.084438  0.069198  0.073911  0.082720  0.074481   \n",
       "3  0.233975  0.222624  0.256220  0.227895  0.203315  0.250494  0.199114   \n",
       "4  0.029336  0.019184  0.023764  0.021166  0.021726  0.027041  0.019323   \n",
       "\n",
       "          7         8  \n",
       "0  0.098372  0.089345  \n",
       "1  0.016930  0.017309  \n",
       "2  0.085460  0.085939  \n",
       "3  0.231126  0.226965  \n",
       "4  0.031685  0.028023  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_knn).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3841977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train.drop('target', axis = 1), \n",
    "                   pd.DataFrame(train_knn, columns = ['knn_1', 'knn_2', 'knn_3', 'knn_4', 'knn_5', 'knn_6', 'knn_7', 'knn_8', 'knn_9'])], axis = 1)\n",
    "test = pd.concat([test.reset_index().drop('id', axis = 1), \n",
    "                   pd.DataFrame(test_knn, columns = ['knn_1', 'knn_2', 'knn_3', 'knn_4', 'knn_5', 'knn_6', 'knn_7', 'knn_8', 'knn_9'])], axis = 1, ignore_index=False)\n",
    "\n",
    "train['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84fc853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train.drop('target', axis =1), targets, test_size = 0.2, stratify = targets, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d891f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_metric(y_true, y_pred):\n",
    "    y_pred = K.clip(y_pred, 1e-15, 1-1e-15)\n",
    "    loss = K.mean(cce(y_true, y_pred))\n",
    "    return loss\n",
    "\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_custom_metric', min_delta=0.00001, patience=6, verbose=0,\n",
    "    mode='min', baseline=None, restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_custom_metric', factor=0.04, patience=5, verbose=0,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a231b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "\n",
    "    #--------- List of hyperparameters --------\n",
    "    # This is example to illustrate how it works. \n",
    "    # Feel free to use list of parameters as you want. Be aware .... the more parameters you specify the more resources (time) it will take\n",
    "    \n",
    "    emb_units = hp.Int('emb_units', min_value = 7, max_value = 8, step = 1)\n",
    "    #conv1d_filters = hp.Int('conv1d_units', min_value = 1, max_value = 2, step = 1)\n",
    "    \n",
    "    dropout_rates = [0.2, 0.4] #[0.2, 0.3, 0.4]\n",
    "    dropout1 = hp.Choice(\"drop_out1\", values = dropout_rates)\n",
    "    dropout2 = hp.Choice(\"drop_out2\", values = dropout_rates)\n",
    "    dropout3 = hp.Float(\"drop_out3\", min_value = 0.0, \n",
    "                        max_value = 0.5, \n",
    "                        default = 0.25, \n",
    "                        step = 0.05,)\n",
    "    \n",
    "    lin_nodes = [16, 64] #[16, 32, 64]\n",
    "    l1_nodes = hp.Choice(\"l1_units\", values = lin_nodes)\n",
    "    l2_nodes = hp.Choice(\"l2_units\", values = lin_nodes)\n",
    "    l3_nodes = hp.Choice(\"l3_units\", values = lin_nodes)\n",
    "    \n",
    "    learning_rates = hp.Choice(\"learning_rate\", [1e-2]) #[1e-2, 1e-3]\n",
    "    \n",
    "    non_linears = ['relu', 'elu'] #['relu', 'selu', 'elu']\n",
    "    act1 = hp.Choice('dense_act1', values = non_linears, default='relu')\n",
    "    act2 = hp.Choice('dense_act2', values = non_linears, default='relu')\n",
    "    act3 = hp.Choice('dense_act3', values = non_linears, default='relu')\n",
    "    \n",
    "    ker_inits = ['lecun_normal', 'he_uniform']\n",
    "    ker_init1 = hp.Choice('kern_init1', values = ker_inits, default = 'lecun_normal')\n",
    "    ker_init2 = hp.Choice('kern_init2', values = ker_inits, default = 'lecun_normal')\n",
    "    ker_init3 = hp.Choice('kern_init3', values = ker_inits, default = 'lecun_normal')\n",
    "    ker_init4 = hp.Choice('kern_init4', values = ker_inits, default = 'lecun_normal')\n",
    "    \n",
    "    conv_kernel = hp.Int('conv_kernel', min_value = 5, max_value = 20, step = 1)\n",
    "    #--------------------------------------\n",
    "    \n",
    "    conv_inputs = layers.Input(shape = (75))\n",
    "    knn_inputs = layers.Input(shape = (9))\n",
    "        \n",
    "    #----------- Embedding layers ----------------------\n",
    "    embed = layers.Embedding (input_dim = 353, \n",
    "                              output_dim = emb_units,\n",
    "                              embeddings_regularizer='l2')(conv_inputs)\n",
    "    \n",
    "    #----------- Convolution layers ----------------------\n",
    "    \n",
    "    embed = layers.Conv1D(conv_kernel, 1, activation = 'relu')(embed) \n",
    "    embed = layers.Flatten()(embed)\n",
    "    hidden = layers.Dropout(dropout1)(embed)\n",
    "    \n",
    "    #----------- Residual blocks layers ----------------------\n",
    "    hidden = tfa.layers.WeightNormalization(\n",
    "                layers.Dense(\n",
    "                units = l1_nodes,\n",
    "                activation = act1, #selu\n",
    "                kernel_initializer = ker_init1))(hidden)\n",
    "   \n",
    "    \n",
    "    output = layers.Dropout(dropout2)(layers.Concatenate()([embed, hidden, knn_inputs]))\n",
    "   \n",
    "    output = tfa.layers.WeightNormalization(\n",
    "    layers.Dense(\n",
    "                units = l2_nodes,\n",
    "                activation = act2,\n",
    "                kernel_initializer = ker_init2))(output) \n",
    "    \n",
    "\n",
    "    output = layers.Dropout(dropout3)(layers.Concatenate()([embed, hidden, output]))\n",
    "    output = tfa.layers.WeightNormalization(\n",
    "    layers.Dense(\n",
    "                units = l3_nodes, \n",
    "                activation = act3, #elu\n",
    "                kernel_initializer = ker_init3))(output)\n",
    "    \n",
    "    #----------- Final layer -----------------------\n",
    "    \n",
    "    conv_outputs = layers.Dense(\n",
    "                units = 9, \n",
    "                activation = 'softmax',\n",
    "                kernel_initializer = ker_init4)(output)\n",
    "    \n",
    "    #----------- Model instantiation  ---------------\n",
    "    model = Model([conv_inputs, knn_inputs],conv_outputs)\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = keras.optimizers.RMSprop(), \n",
    "                  metrics = custom_metric)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bfd1194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = BayesianOptimization(\n",
    "    model_builder,\n",
    "    objective = \"val_loss\",\n",
    "    max_trials = 100, # This is only demo - you can play with more trials on local machine (Kaggle is resource limited). I usually run from 100-1000 trials for best params.\n",
    "    executions_per_trial = 2,\n",
    "    overwrite = True,\n",
    "    seed = RANDOM_STATE,\n",
    "    directory = \"tps-06\",\n",
    "    project_name = \"nn-embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bc6ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 16\n",
      "emb_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 7, 'max_value': 8, 'step': 1, 'sampling': None}\n",
      "drop_out1 (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.4], 'ordered': True}\n",
      "drop_out2 (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.4], 'ordered': True}\n",
      "drop_out3 (Float)\n",
      "{'default': 0.25, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "l1_units (Choice)\n",
      "{'default': 16, 'conditions': [], 'values': [16, 64], 'ordered': True}\n",
      "l2_units (Choice)\n",
      "{'default': 16, 'conditions': [], 'values': [16, 64], 'ordered': True}\n",
      "l3_units (Choice)\n",
      "{'default': 16, 'conditions': [], 'values': [16, 64], 'ordered': True}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01], 'ordered': True}\n",
      "dense_act1 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'elu'], 'ordered': False}\n",
      "dense_act2 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'elu'], 'ordered': False}\n",
      "dense_act3 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'elu'], 'ordered': False}\n",
      "kern_init1 (Choice)\n",
      "{'default': 'lecun_normal', 'conditions': [], 'values': ['lecun_normal', 'he_uniform'], 'ordered': False}\n",
      "kern_init2 (Choice)\n",
      "{'default': 'lecun_normal', 'conditions': [], 'values': ['lecun_normal', 'he_uniform'], 'ordered': False}\n",
      "kern_init3 (Choice)\n",
      "{'default': 'lecun_normal', 'conditions': [], 'values': ['lecun_normal', 'he_uniform'], 'ordered': False}\n",
      "kern_init4 (Choice)\n",
      "{'default': 'lecun_normal', 'conditions': [], 'values': ['lecun_normal', 'he_uniform'], 'ordered': False}\n",
      "conv_kernel (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 5, 'max_value': 20, 'step': 1, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d622603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs 2-5 is the best in this competition\n",
    "if not (OPTIM_MODE == 3):\n",
    "    tuner.search([X_train.iloc[:, :75], X_train.iloc[:, 75:]], y_train, epochs = 3, validation_data = ([X_valid.iloc[:, :75], X_valid.iloc[:, 75:]], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29eab7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (OPTIM_MODE == 3):\n",
    "    tuner.results_summary(num_trials = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d67624cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (OPTIM_MODE == 3):\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    model = tuner.hypermodel.build(best_hp)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5e92027",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (OPTIM_MODE == 3):\n",
    "    plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3c3085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are list of TOP3 params found during my research (1000 trials)   \n",
    "net_params = [{'emb_units': 8, 'conv1d_units': 1, \n",
    "               'drop_out1': 0.3, 'drop_out2': 0.4, 'drop_out3': 0.2, \n",
    "               'l1_units': 16, 'l2_units': 64, 'l3_units': 16, \n",
    "               'learning_rate': 0.001, \n",
    "               'dense_act1': 'elu', 'dense_act2': 'relu', 'dense_act3': 'relu',\n",
    "              'kern_init1': 'he_uniform', 'kern_init2': 'he_uniform', 'kern_init3': 'he_uniform', 'kern_init4': 'lecun_normal'},\n",
    "              {'emb_units': 8, 'conv1d_units': 1, \n",
    "               'drop_out1': 0.3, 'drop_out2': 0.4, 'drop_out3': 0.2, \n",
    "               'l1_units': 16, 'l2_units': 64, 'l3_units': 16, \n",
    "               'learning_rate': 0.001, \n",
    "               'dense_act1': 'elu', 'dense_act2': 'relu', 'dense_act3': 'relu',\n",
    "              'kern_init1': 'he_uniform', 'kern_init2': 'he_uniform', 'kern_init3': 'he_uniform', 'kern_init4': 'lecun_normal'},\n",
    "              {'emb_units': 7, 'conv1d_units': 1, \n",
    "               'drop_out1': 0.3, 'drop_out2': 0.2, 'drop_out3': 0.2, \n",
    "               'l1_units': 16, 'l2_units': 128, 'l3_units': 32, \n",
    "               'learning_rate': 0.001, \n",
    "               'dense_act1': 'elu', 'dense_act2': 'relu', 'dense_act3': 'relu',\n",
    "              'kern_init1': 'he_uniform', 'kern_init2': 'he_uniform', 'kern_init3': 'he_uniform', 'kern_init4': 'lecun_normal'}\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc40a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder_optimized(net_config):\n",
    " \n",
    "    emb_units = net_config['emb_units']\n",
    "    conv1d_filters = net_config['conv1d_units']\n",
    "    \n",
    "    dropout1 = net_config[\"drop_out1\"]\n",
    "    dropout2 = net_config[\"drop_out2\"]\n",
    "    dropout3 = net_config[\"drop_out3\"]\n",
    "\n",
    "    l1_nodes = net_config[\"l1_units\"]\n",
    "    l2_nodes = net_config[\"l2_units\"]\n",
    "    l3_nodes = net_config[\"l3_units\"]\n",
    "    \n",
    "    learning_rates = net_config[\"learning_rate\"]\n",
    "\n",
    "    act1 = net_config['dense_act1']\n",
    "    act2 = net_config['dense_act2']\n",
    "    act3 = net_config['dense_act3']\n",
    "    \n",
    "\n",
    "    ker_init1 = net_config['kern_init1']\n",
    "    ker_init2 = net_config['kern_init2']\n",
    "    ker_init3 = net_config['kern_init3']\n",
    "    ker_init4 = net_config['kern_init4']\n",
    "    #--------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    conv_inputs = layers.Input(shape = (75))\n",
    "    knn_inputs = layers.Input(shape = (9))\n",
    "    #----------- Embedding layers ----------------------\n",
    "    embed = layers.Embedding (input_dim = 353, \n",
    "                              output_dim = emb_units,\n",
    "                              embeddings_regularizer='l2')(conv_inputs)\n",
    "    \n",
    "    #----------- Convolution layers ----------------------\n",
    "    \n",
    "    embed = layers.Conv1D(10, conv1d_filters, activation = 'relu')(embed) \n",
    "    embed = layers.Flatten()(embed)\n",
    "    hidden = layers.Dropout(dropout1)(embed)\n",
    "    \n",
    "    #----------- Residual blocks layers ----------------------\n",
    "    hidden = tfa.layers.WeightNormalization(\n",
    "                layers.Dense(\n",
    "                units = l1_nodes,\n",
    "                activation = act1, #selu\n",
    "                kernel_initializer = ker_init1))(hidden)\n",
    "    \n",
    "   \n",
    "    output = layers.Dropout(dropout2)(layers.Concatenate()([embed, hidden, knn_inputs]))\n",
    "   \n",
    "    output = tfa.layers.WeightNormalization(\n",
    "    layers.Dense(\n",
    "                units = l2_nodes,\n",
    "                activation = act2,\n",
    "                kernel_initializer = ker_init2))(output) \n",
    "    \n",
    "\n",
    "    output = layers.Dropout(dropout3)(layers.Concatenate()([embed, hidden, output]))\n",
    "    output = tfa.layers.WeightNormalization(\n",
    "    layers.Dense(\n",
    "                units = l3_nodes, \n",
    "                activation = act3, #elu\n",
    "                kernel_initializer = ker_init3))(output)\n",
    "    \n",
    "    \n",
    "    #----------- Final layer -----------------------\n",
    "    \n",
    "    conv_outputs = layers.Dense(\n",
    "                units = 9, \n",
    "                activation = 'softmax',\n",
    "                kernel_initializer = ker_init4)(output)\n",
    "    \n",
    "    #----------- Model instantiation  ---------------\n",
    "    model = Model([conv_inputs, knn_inputs], conv_outputs)\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = keras.optimizers.Adam(learning_rates), \n",
    "                  metrics = custom_metric)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33d27d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_class_optimizer(weights, a0, a1, a2, a3, a4, a5, a6, a7, a8):\n",
    "    oof = np.array([weights[0]*a0, \n",
    "                    weights[1]*a1, \n",
    "                    weights[2]*a2, \n",
    "                    weights[3]*a3, \n",
    "                    weights[4]*a4, \n",
    "                    weights[5]*a5, \n",
    "                    weights[6]*a6, \n",
    "                    weights[7]*a7, \n",
    "                    weights[8]*a8]).transpose()\n",
    "    \n",
    "    oof = oof / np.sum(oof, axis=1).reshape(-1, 1)\n",
    "    return log_loss(y_val, oof)\n",
    "\n",
    "\n",
    "def pred_fold_optimizer(oof_preds, test_preds):\n",
    "    \n",
    "    cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "    res = minimize(fun=inter_class_optimizer,\n",
    "                   x0=[1/9 for _ in range(9)],\n",
    "                   args=tuple(oof_preds[ :, i] for i in range(9)),\n",
    "                   method= 'Nelder-Mead',\n",
    "                   options={'maxiter': 300},\n",
    "                   bounds=[(0.0, 1.0)] * len(oof_class_preds),\n",
    "                   constraints=cons)\n",
    "\n",
    "    oof_preds = np.array([res.x[i]*oof_preds[ :, i] for i in range(9)]).transpose()\n",
    "    oof_preds = oof_preds / np.sum(oof_preds, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    test_preds = np.array([res.x[i]*test_preds[:, i] for i in range(9)]).transpose()\n",
    "    test_preds = test_preds / np.sum(test_preds, axis=1).reshape(-1, 1)\n",
    "\n",
    "    return res[\"fun\"], test_preds, oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d3c318b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_model_optimizer(weights):\n",
    "    final_prediction = 0\n",
    "    \n",
    "    for weight, prediction in zip(weights, oof_class_preds):\n",
    "        final_prediction += weight * prediction\n",
    "    \n",
    "    return log_loss(y_val, final_prediction)\n",
    "\n",
    "def pred_model_optimizer(oof_class_preds, test_class_preds):\n",
    "    optmized_oof_nn_preds = 0\n",
    "    optmized_test_nn_preds = 0\n",
    "    \n",
    "    starting_values = [1/len(oof_class_preds)] * len(oof_class_preds)\n",
    "    \n",
    "    cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "    res = minimize(inter_model_optimizer, \n",
    "                   starting_values,\n",
    "                   method='Nelder-Mead',\n",
    "                   bounds=[(0.0, 1.0)] * len(oof_class_preds),\n",
    "                   constraints=cons)\n",
    "    \n",
    "    print(f'--- Inter model optimized logloss: {(res[\"fun\"]):.5f} using {res[\"x\"]} weights (sum:{np.sum(res[\"x\"])}) ---\\n')\n",
    "\n",
    "    for weight, prediction in zip(res[\"x\"], oof_class_preds):\n",
    "        optmized_oof_nn_preds += weight * prediction\n",
    "    \n",
    "    for weight, prediction in zip(res[\"x\"], test_class_preds):\n",
    "        optmized_test_nn_preds += weight * prediction\n",
    "\n",
    "        \n",
    "    return optmized_oof_nn_preds, optmized_test_nn_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2957ba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Training and blending 90 models -----\n",
      "\n",
      "- RANDOM STATE 2021 -\n",
      "  * FOLD 1 -> MODEL 1 -> SCORE: 1.74064 -> OPTIMIZED SCORE: 1.74062 (GAIN: -0.00003)\n",
      "  * FOLD 1 -> MODEL 2 -> SCORE: 1.74106 -> OPTIMIZED SCORE: 1.74103 (GAIN: -0.00003)\n",
      "  * FOLD 1 -> MODEL 3 -> SCORE: 1.74007 -> OPTIMIZED SCORE: 1.74006 (GAIN: -0.00001)\n",
      "--- Inter model optimized logloss: 1.73918 using [0.29519231 0.23384731 0.46391193] weights (sum:0.9929515598267288) ---\n",
      "\n",
      "  * FOLD 2 -> MODEL 1 -> SCORE: 1.73752 -> OPTIMIZED SCORE: 1.73745 (GAIN: -0.00007)\n",
      "  * FOLD 2 -> MODEL 2 -> SCORE: 1.73834 -> OPTIMIZED SCORE: 1.73831 (GAIN: -0.00004)\n",
      "  * FOLD 2 -> MODEL 3 -> SCORE: 1.73861 -> OPTIMIZED SCORE: 1.73854 (GAIN: -0.00008)\n",
      "--- Inter model optimized logloss: 1.73685 using [0.51239596 0.24313905 0.24758235] weights (sum:1.0031173562413775) ---\n",
      "\n",
      "  * FOLD 3 -> MODEL 1 -> SCORE: 1.74459 -> OPTIMIZED SCORE: 1.74382 (GAIN: -0.00077)\n",
      "  * FOLD 3 -> MODEL 2 -> SCORE: 1.74368 -> OPTIMIZED SCORE: 1.74361 (GAIN: -0.00008)\n",
      "  * FOLD 3 -> MODEL 3 -> SCORE: 1.74412 -> OPTIMIZED SCORE: 1.74406 (GAIN: -0.00006)\n",
      "--- Inter model optimized logloss: 1.74227 using [0.36442324 0.33215843 0.30370378] weights (sum:1.000285446851192) ---\n",
      "\n",
      "  * FOLD 4 -> MODEL 1 -> SCORE: 1.74307 -> OPTIMIZED SCORE: 1.74302 (GAIN: -0.00005)\n",
      "  * FOLD 4 -> MODEL 2 -> SCORE: 1.74252 -> OPTIMIZED SCORE: 1.74248 (GAIN: -0.00003)\n",
      "  * FOLD 4 -> MODEL 3 -> SCORE: 1.74284 -> OPTIMIZED SCORE: 1.74239 (GAIN: -0.00045)\n",
      "--- Inter model optimized logloss: 1.74151 using [0.22999162 0.32802249 0.42219147] weights (sum:0.9802055882808343) ---\n",
      "\n",
      "  * FOLD 5 -> MODEL 1 -> SCORE: 1.73955 -> OPTIMIZED SCORE: 1.73954 (GAIN: -0.00001)\n",
      "  * FOLD 5 -> MODEL 2 -> SCORE: 1.73916 -> OPTIMIZED SCORE: 1.73915 (GAIN: -0.00001)\n",
      "  * FOLD 5 -> MODEL 3 -> SCORE: 1.73971 -> OPTIMIZED SCORE: 1.73930 (GAIN: -0.00041)\n",
      "--- Inter model optimized logloss: 1.73816 using [0.24660568 0.37612853 0.35971167] weights (sum:0.9824458819508834) ---\n",
      "\n",
      "  * FOLD 6 -> MODEL 1 -> SCORE: 1.74240 -> OPTIMIZED SCORE: 1.74236 (GAIN: -0.00004)\n",
      "  * FOLD 6 -> MODEL 2 -> SCORE: 1.74294 -> OPTIMIZED SCORE: 1.74230 (GAIN: -0.00064)\n",
      "  * FOLD 6 -> MODEL 3 -> SCORE: 1.74295 -> OPTIMIZED SCORE: 1.74214 (GAIN: -0.00081)\n",
      "--- Inter model optimized logloss: 1.74102 using [0.28851262 0.35239338 0.36024846] weights (sum:1.0011544659767393) ---\n",
      "\n",
      "  * FOLD 7 -> MODEL 1 -> SCORE: 1.73791 -> OPTIMIZED SCORE: 1.73785 (GAIN: -0.00006)\n",
      "  * FOLD 7 -> MODEL 2 -> SCORE: 1.73729 -> OPTIMIZED SCORE: 1.73724 (GAIN: -0.00004)\n",
      "  * FOLD 7 -> MODEL 3 -> SCORE: 1.73841 -> OPTIMIZED SCORE: 1.73793 (GAIN: -0.00048)\n",
      "--- Inter model optimized logloss: 1.73639 using [0.23383914 0.42554762 0.32021929] weights (sum:0.9796060439228038) ---\n",
      "\n",
      "  * FOLD 8 -> MODEL 1 -> SCORE: 1.74359 -> OPTIMIZED SCORE: 1.74282 (GAIN: -0.00077)\n",
      "  * FOLD 8 -> MODEL 2 -> SCORE: 1.74207 -> OPTIMIZED SCORE: 1.74193 (GAIN: -0.00014)\n",
      "  * FOLD 8 -> MODEL 3 -> SCORE: 1.74125 -> OPTIMIZED SCORE: 1.74058 (GAIN: -0.00066)\n",
      "--- Inter model optimized logloss: 1.74008 using [0.14448865 0.2181222  0.52622258] weights (sum:0.8888334303902583) ---\n",
      "\n",
      "  * FOLD 9 -> MODEL 1 -> SCORE: 1.74233 -> OPTIMIZED SCORE: 1.74224 (GAIN: -0.00010)\n",
      "  * FOLD 9 -> MODEL 2 -> SCORE: 1.74327 -> OPTIMIZED SCORE: 1.74270 (GAIN: -0.00057)\n",
      "  * FOLD 9 -> MODEL 3 -> SCORE: 1.74318 -> OPTIMIZED SCORE: 1.74267 (GAIN: -0.00051)\n",
      "--- Inter model optimized logloss: 1.74125 using [0.3788827  0.29264951 0.32166681] weights (sum:0.9931990165227419) ---\n",
      "\n",
      "  * FOLD 10 -> MODEL 1 -> SCORE: 1.74855 -> OPTIMIZED SCORE: 1.74851 (GAIN: -0.00004)\n",
      "  * FOLD 10 -> MODEL 2 -> SCORE: 1.74896 -> OPTIMIZED SCORE: 1.74886 (GAIN: -0.00009)\n",
      "  * FOLD 10 -> MODEL 3 -> SCORE: 1.74838 -> OPTIMIZED SCORE: 1.74824 (GAIN: -0.00013)\n",
      "--- Inter model optimized logloss: 1.74746 using [0.38506393 0.19016466 0.41325903] weights (sum:0.9884876167321872) ---\n",
      "\n",
      "- FINAL SCORE FOR 3 MODELS IN RANDOM STATE 2021: 1.74054 - OPTIMIZED (inter class and model): 1.74042 (GAIN: -0.00013)\n",
      "\n",
      "- RANDOM STATE 2022 -\n",
      "  * FOLD 1 -> MODEL 1 -> SCORE: 1.74500 -> OPTIMIZED SCORE: 1.74492 (GAIN: -0.00008)\n",
      "  * FOLD 1 -> MODEL 2 -> SCORE: 1.74497 -> OPTIMIZED SCORE: 1.74494 (GAIN: -0.00002)\n",
      "  * FOLD 1 -> MODEL 3 -> SCORE: 1.74654 -> OPTIMIZED SCORE: 1.74606 (GAIN: -0.00048)\n",
      "--- Inter model optimized logloss: 1.74395 using [0.43201232 0.36032454 0.16890107] weights (sum:0.9612379218183413) ---\n",
      "\n",
      "  * FOLD 2 -> MODEL 1 -> SCORE: 1.74100 -> OPTIMIZED SCORE: 1.74057 (GAIN: -0.00042)\n",
      "  * FOLD 2 -> MODEL 2 -> SCORE: 1.74112 -> OPTIMIZED SCORE: 1.74107 (GAIN: -0.00005)\n",
      "  * FOLD 2 -> MODEL 3 -> SCORE: 1.74171 -> OPTIMIZED SCORE: 1.74131 (GAIN: -0.00040)\n",
      "--- Inter model optimized logloss: 1.73952 using [0.44615383 0.2598529  0.30090171] weights (sum:1.0069084386743046) ---\n",
      "\n",
      "  * FOLD 3 -> MODEL 1 -> SCORE: 1.73792 -> OPTIMIZED SCORE: 1.73785 (GAIN: -0.00007)\n",
      "  * FOLD 3 -> MODEL 2 -> SCORE: 1.73868 -> OPTIMIZED SCORE: 1.73866 (GAIN: -0.00002)\n",
      "  * FOLD 3 -> MODEL 3 -> SCORE: 1.73892 -> OPTIMIZED SCORE: 1.73861 (GAIN: -0.00031)\n",
      "--- Inter model optimized logloss: 1.73688 using [0.45409458 0.19406863 0.2979963 ] weights (sum:0.9461595083889417) ---\n",
      "\n",
      "  * FOLD 4 -> MODEL 1 -> SCORE: 1.73787 -> OPTIMIZED SCORE: 1.73780 (GAIN: -0.00007)\n",
      "  * FOLD 4 -> MODEL 2 -> SCORE: 1.73840 -> OPTIMIZED SCORE: 1.73789 (GAIN: -0.00051)\n",
      "  * FOLD 4 -> MODEL 3 -> SCORE: 1.73874 -> OPTIMIZED SCORE: 1.73872 (GAIN: -0.00002)\n",
      "--- Inter model optimized logloss: 1.73686 using [0.35683699 0.40443585 0.22158432] weights (sum:0.9828571517507797) ---\n",
      "\n",
      "  * FOLD 5 -> MODEL 1 -> SCORE: 1.73781 -> OPTIMIZED SCORE: 1.73755 (GAIN: -0.00026)\n",
      "  * FOLD 5 -> MODEL 2 -> SCORE: 1.73750 -> OPTIMIZED SCORE: 1.73685 (GAIN: -0.00065)\n",
      "  * FOLD 5 -> MODEL 3 -> SCORE: 1.73751 -> OPTIMIZED SCORE: 1.73672 (GAIN: -0.00078)\n",
      "--- Inter model optimized logloss: 1.73585 using [0.15474618 0.38932326 0.43585098] weights (sum:0.9799204084979706) ---\n",
      "\n",
      "  * FOLD 6 -> MODEL 1 -> SCORE: 1.74482 -> OPTIMIZED SCORE: 1.74424 (GAIN: -0.00058)\n",
      "  * FOLD 6 -> MODEL 2 -> SCORE: 1.74631 -> OPTIMIZED SCORE: 1.74558 (GAIN: -0.00073)\n",
      "  * FOLD 6 -> MODEL 3 -> SCORE: 1.74662 -> OPTIMIZED SCORE: 1.74588 (GAIN: -0.00074)\n",
      "--- Inter model optimized logloss: 1.74369 using [0.52974575 0.22546467 0.1545917 ] weights (sum:0.9098021231301547) ---\n",
      "\n",
      "  * FOLD 7 -> MODEL 1 -> SCORE: 1.74329 -> OPTIMIZED SCORE: 1.74320 (GAIN: -0.00009)\n",
      "  * FOLD 7 -> MODEL 2 -> SCORE: 1.74369 -> OPTIMIZED SCORE: 1.74357 (GAIN: -0.00012)\n",
      "  * FOLD 7 -> MODEL 3 -> SCORE: 1.74432 -> OPTIMIZED SCORE: 1.74385 (GAIN: -0.00048)\n",
      "--- Inter model optimized logloss: 1.74219 using [0.4219243  0.27934939 0.30862931] weights (sum:1.0099029969569027) ---\n",
      "\n",
      "  * FOLD 8 -> MODEL 1 -> SCORE: 1.74224 -> OPTIMIZED SCORE: 1.74219 (GAIN: -0.00005)\n",
      "  * FOLD 8 -> MODEL 2 -> SCORE: 1.74223 -> OPTIMIZED SCORE: 1.74220 (GAIN: -0.00003)\n",
      "  * FOLD 8 -> MODEL 3 -> SCORE: 1.74356 -> OPTIMIZED SCORE: 1.74233 (GAIN: -0.00123)\n",
      "--- Inter model optimized logloss: 1.74122 using [0.31820436 0.34879245 0.34676999] weights (sum:1.0137668050598252) ---\n",
      "\n",
      "  * FOLD 9 -> MODEL 1 -> SCORE: 1.74240 -> OPTIMIZED SCORE: 1.74211 (GAIN: -0.00029)\n",
      "  * FOLD 9 -> MODEL 2 -> SCORE: 1.74219 -> OPTIMIZED SCORE: 1.74213 (GAIN: -0.00007)\n",
      "  * FOLD 9 -> MODEL 3 -> SCORE: 1.74275 -> OPTIMIZED SCORE: 1.74236 (GAIN: -0.00039)\n",
      "--- Inter model optimized logloss: 1.74094 using [0.34958945 0.31360241 0.35153318] weights (sum:1.014725028393332) ---\n",
      "\n",
      "  * FOLD 10 -> MODEL 1 -> SCORE: 1.74407 -> OPTIMIZED SCORE: 1.74399 (GAIN: -0.00008)\n",
      "  * FOLD 10 -> MODEL 2 -> SCORE: 1.74319 -> OPTIMIZED SCORE: 1.74318 (GAIN: -0.00001)\n",
      "  * FOLD 10 -> MODEL 3 -> SCORE: 1.74532 -> OPTIMIZED SCORE: 1.74470 (GAIN: -0.00062)\n",
      "--- Inter model optimized logloss: 1.74263 using [0.38453491 0.84722836 0.28252452] weights (sum:1.5142877932328616) ---\n",
      "\n",
      "- FINAL SCORE FOR 3 MODELS IN RANDOM STATE 2022: 1.74056 - OPTIMIZED (inter class and model): 1.74037 (GAIN: -0.00019)\n",
      "\n",
      "- RANDOM STATE 2023 -\n",
      "  * FOLD 1 -> MODEL 1 -> SCORE: 1.74410 -> OPTIMIZED SCORE: 1.74403 (GAIN: -0.00007)\n",
      "  * FOLD 1 -> MODEL 2 -> SCORE: 1.74381 -> OPTIMIZED SCORE: 1.74361 (GAIN: -0.00020)\n",
      "  * FOLD 1 -> MODEL 3 -> SCORE: 1.74567 -> OPTIMIZED SCORE: 1.74501 (GAIN: -0.00067)\n",
      "--- Inter model optimized logloss: 1.74287 using [0.3097     0.42366182 0.22657826] weights (sum:0.9599400867964125) ---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * FOLD 2 -> MODEL 1 -> SCORE: 1.74250 -> OPTIMIZED SCORE: 1.74194 (GAIN: -0.00056)\n",
      "  * FOLD 2 -> MODEL 2 -> SCORE: 1.74125 -> OPTIMIZED SCORE: 1.74121 (GAIN: -0.00003)\n",
      "  * FOLD 2 -> MODEL 3 -> SCORE: 1.74222 -> OPTIMIZED SCORE: 1.74220 (GAIN: -0.00002)\n",
      "--- Inter model optimized logloss: 1.74002 using [0.29390235 0.37564475 0.3386028 ] weights (sum:1.0081498911036517) ---\n",
      "\n",
      "  * FOLD 3 -> MODEL 1 -> SCORE: 1.73191 -> OPTIMIZED SCORE: 1.73187 (GAIN: -0.00004)\n",
      "  * FOLD 3 -> MODEL 2 -> SCORE: 1.73216 -> OPTIMIZED SCORE: 1.73212 (GAIN: -0.00004)\n",
      "  * FOLD 3 -> MODEL 3 -> SCORE: 1.73167 -> OPTIMIZED SCORE: 1.73167 (GAIN: -0.00001)\n",
      "--- Inter model optimized logloss: 1.73077 using [0.29045546 0.30307765 0.40543768] weights (sum:0.998970787654125) ---\n",
      "\n",
      "  * FOLD 4 -> MODEL 1 -> SCORE: 1.74405 -> OPTIMIZED SCORE: 1.74400 (GAIN: -0.00004)\n",
      "  * FOLD 4 -> MODEL 2 -> SCORE: 1.74421 -> OPTIMIZED SCORE: 1.74414 (GAIN: -0.00008)\n",
      "  * FOLD 4 -> MODEL 3 -> SCORE: 1.74552 -> OPTIMIZED SCORE: 1.74547 (GAIN: -0.00005)\n",
      "--- Inter model optimized logloss: 1.74303 using [0.39360261 0.38334795 0.20701462] weights (sum:0.9839651740658306) ---\n",
      "\n",
      "  * FOLD 5 -> MODEL 1 -> SCORE: 1.74557 -> OPTIMIZED SCORE: 1.74506 (GAIN: -0.00051)\n",
      "  * FOLD 5 -> MODEL 2 -> SCORE: 1.74601 -> OPTIMIZED SCORE: 1.74590 (GAIN: -0.00011)\n",
      "  * FOLD 5 -> MODEL 3 -> SCORE: 1.74576 -> OPTIMIZED SCORE: 1.74541 (GAIN: -0.00035)\n",
      "--- Inter model optimized logloss: 1.74419 using [0.40517115 0.2504318  0.33630597] weights (sum:0.9919089141734277) ---\n",
      "\n",
      "  * FOLD 6 -> MODEL 1 -> SCORE: 1.74357 -> OPTIMIZED SCORE: 1.74347 (GAIN: -0.00010)\n",
      "  * FOLD 6 -> MODEL 2 -> SCORE: 1.74297 -> OPTIMIZED SCORE: 1.74294 (GAIN: -0.00003)\n",
      "  * FOLD 6 -> MODEL 3 -> SCORE: 1.74463 -> OPTIMIZED SCORE: 1.74410 (GAIN: -0.00053)\n",
      "--- Inter model optimized logloss: 1.74225 using [0.28754396 0.42945682 0.20141361] weights (sum:0.9184143927853535) ---\n",
      "\n",
      "  * FOLD 7 -> MODEL 1 -> SCORE: 1.74693 -> OPTIMIZED SCORE: 1.74688 (GAIN: -0.00005)\n",
      "  * FOLD 7 -> MODEL 2 -> SCORE: 1.74677 -> OPTIMIZED SCORE: 1.74671 (GAIN: -0.00006)\n",
      "  * FOLD 7 -> MODEL 3 -> SCORE: 1.74642 -> OPTIMIZED SCORE: 1.74569 (GAIN: -0.00072)\n",
      "--- Inter model optimized logloss: 1.74509 using [0.22849923 0.22215784 0.55331027] weights (sum:1.0039673473809105) ---\n",
      "\n",
      "  * FOLD 8 -> MODEL 1 -> SCORE: 1.73474 -> OPTIMIZED SCORE: 1.73473 (GAIN: -0.00001)\n",
      "  * FOLD 8 -> MODEL 2 -> SCORE: 1.73416 -> OPTIMIZED SCORE: 1.73408 (GAIN: -0.00009)\n",
      "  * FOLD 8 -> MODEL 3 -> SCORE: 1.73394 -> OPTIMIZED SCORE: 1.73389 (GAIN: -0.00005)\n",
      "--- Inter model optimized logloss: 1.73301 using [0.1887062  0.36014329 0.42249753] weights (sum:0.9713470149093378) ---\n",
      "\n",
      "  * FOLD 9 -> MODEL 1 -> SCORE: 1.74034 -> OPTIMIZED SCORE: 1.74026 (GAIN: -0.00009)\n",
      "  * FOLD 9 -> MODEL 2 -> SCORE: 1.74027 -> OPTIMIZED SCORE: 1.74022 (GAIN: -0.00005)\n",
      "  * FOLD 9 -> MODEL 3 -> SCORE: 1.74259 -> OPTIMIZED SCORE: 1.74210 (GAIN: -0.00049)\n",
      "--- Inter model optimized logloss: 1.73938 using [0.41367153 0.41976331 0.1037184 ] weights (sum:0.9371532433125287) ---\n",
      "\n",
      "  * FOLD 10 -> MODEL 1 -> SCORE: 1.74403 -> OPTIMIZED SCORE: 1.74392 (GAIN: -0.00011)\n",
      "  * FOLD 10 -> MODEL 2 -> SCORE: 1.74369 -> OPTIMIZED SCORE: 1.74336 (GAIN: -0.00033)\n",
      "  * FOLD 10 -> MODEL 3 -> SCORE: 1.74500 -> OPTIMIZED SCORE: 1.74433 (GAIN: -0.00066)\n",
      "--- Inter model optimized logloss: 1.74250 using [0.24832357 0.4795422  0.28186343] weights (sum:1.0097291931795556) ---\n",
      "\n",
      "- FINAL SCORE FOR 3 MODELS IN RANDOM STATE 2023: 1.74046 - OPTIMIZED (inter class and model): 1.74031 (GAIN: -0.00015)\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 70\n",
    "\n",
    "N_FOLDS = 10\n",
    "RANDOM_STATES_NUM = 3\n",
    "NUM_TOP_MODELS = 3\n",
    "\n",
    "y_val = []\n",
    "pred_NN_a = np.zeros((test.shape[0],9))\n",
    "pred_NN_a_optimized = np.zeros((test.shape[0],9))\n",
    "\n",
    "\n",
    "if not (OPTIM_MODE == 1): \n",
    "    tuners = tuner.get_best_hyperparameters(num_trials = NUM_TOP_MODELS)\n",
    "    print(f'----- Training and blending {N_FOLDS * RANDOM_STATES_NUM * NUM_TOP_MODELS} models -----')\n",
    "\n",
    "    for rs_n in range(RANDOM_STATES_NUM):\n",
    "        print(F\"\\n- RANDOM STATE {RANDOM_STATE + rs_n} -\")\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state = (RANDOM_STATE + rs_n))\n",
    "\n",
    "        oof_NN_a = np.zeros((train.shape[0],9))\n",
    "        oof_NN_a_optim = np.zeros((train.shape[0],9))\n",
    "        oof_NN_fold_optimized = np.zeros((train.shape[0],9))\n",
    "       \n",
    "        for fold, (tr_idx, ts_idx) in enumerate(skf.split(train, train.target)):\n",
    "\n",
    "            X_train = train.iloc[:, :75].iloc[tr_idx]\n",
    "            X_train_knn = train.iloc[:, 75:-1].iloc[tr_idx]\n",
    "            y_train = targets.iloc[tr_idx]\n",
    "            \n",
    "            X_test = train.iloc[:, :75].iloc[ts_idx]\n",
    "            X_test_knn = train.iloc[:, 75:-1].iloc[ts_idx]\n",
    "            y_test = targets.iloc[ts_idx]\n",
    "            \n",
    "            oof_class_preds = []\n",
    "            test_class_preds = []\n",
    "\n",
    "            for n_models in range(NUM_TOP_MODELS):\n",
    "\n",
    "                K.clear_session()  \n",
    "\n",
    "                if OPTIM_MODE == 2:\n",
    "                    params = tuners[n_models]   \n",
    "                    model_conv = tuner.hypermodel.build(params)\n",
    "                    l_rate = best_hp.get('learning_rate')\n",
    "                else:\n",
    "                    model_conv = model_builder_optimized(net_params[n_models])\n",
    "                    l_rate = net_params[n_models][\"learning_rate\"]\n",
    "\n",
    "                model_conv.compile(loss='categorical_crossentropy', \n",
    "                                        optimizer = keras.optimizers.Adam(learning_rate = l_rate), \n",
    "                                        metrics=custom_metric)\n",
    "\n",
    "                model_conv.fit([X_train, X_train_knn], y_train,\n",
    "                          batch_size = 128, epochs = EPOCH,\n",
    "                          validation_data=([X_test, X_test_knn], y_test),\n",
    "                          callbacks=[es, plateau],\n",
    "                          verbose = 0)\n",
    "\n",
    "                pred_a = model_conv.predict([X_test, X_test_knn]) \n",
    "                score_NN_a = log_loss(y_test, pred_a)  \n",
    "                \n",
    "                test_NN_preds = model_conv.predict([test.iloc[:, :75], test.iloc[:, 75:]]) \n",
    "                \n",
    "                y_val = target_optim.iloc[ts_idx]\n",
    "                optim_score, test_preds_optim, oof_preds_optim = pred_fold_optimizer(pred_a, test_NN_preds)\n",
    "                 \n",
    "                print(f\"  * FOLD {fold + 1} -> MODEL {n_models + 1} -> SCORE: {(score_NN_a):.5f} -> OPTIMIZED SCORE: {optim_score:.5f} (GAIN: {(optim_score-score_NN_a):.5f})\")\n",
    "                \n",
    "                pred_NN_a += test_preds_optim\n",
    "                oof_NN_a[ts_idx] += pred_a \n",
    "                oof_NN_a_optim[ts_idx] += oof_preds_optim \n",
    "                \n",
    "                # ---\n",
    "                oof_class_preds.append(oof_preds_optim)\n",
    "                test_class_preds.append(test_preds_optim)   \n",
    "                # ---\n",
    "        \n",
    "            oof_NN_fold_optimized[ts_idx], pred_NN_optimized = pred_model_optimizer(oof_class_preds, test_class_preds)\n",
    "            pred_NN_a_optimized += pred_NN_optimized\n",
    "\n",
    "        score_a = log_loss(targets, (oof_NN_a / NUM_TOP_MODELS))\n",
    "        score_o = log_loss(targets, oof_NN_fold_optimized)\n",
    "        print(f\"- FINAL SCORE FOR {n_models + 1} MODELS IN RANDOM STATE {RANDOM_STATE + rs_n}: {score_a:.5f} - OPTIMIZED (inter class and model): {score_o:.5f} (GAIN: {(score_o-score_a):.5f})\")\n",
    "\n",
    "    pred_NN_a = pred_NN_a / (N_FOLDS * RANDOM_STATES_NUM * NUM_TOP_MODELS)\n",
    "    pred_NN_a_optimized = pred_NN_a_optimized /  (N_FOLDS * RANDOM_STATES_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b6a3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (OPTIM_MODE == 1):\n",
    "    pred_embedding = pred_NN_a_optimized\n",
    "    submission['Class_1']=pred_embedding[:,0]\n",
    "    submission['Class_2']=pred_embedding[:,1]\n",
    "    submission['Class_3']=pred_embedding[:,2]\n",
    "    submission['Class_4']=pred_embedding[:,3]\n",
    "    submission['Class_5']=pred_embedding[:,4]\n",
    "    submission['Class_6']=pred_embedding[:,5]\n",
    "    submission['Class_7']=pred_embedding[:,6]\n",
    "    submission['Class_8']=pred_embedding[:,7]\n",
    "    submission['Class_9']=pred_embedding[:,8]\n",
    "\n",
    "    submission.to_csv(\"26-tps06-keras-tuner.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
